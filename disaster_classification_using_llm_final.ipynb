{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d12cc509-d236-4ad1-8376-fd82fce48fee",
   "metadata": {},
   "source": [
    "# Disaster Classification Using Large Language Models (LLMs)\n",
    "This project leverages three different popular LLMs such as BERT, RoBERTa and DistilBERT to classify different types of disasters from tweets. In addition, a majority voting based classification of disasters from tweets have been implemented in this project where we chose the best performing models after applying three different fine-tuning approaches such as - standard fine-tuning, LoRA and few-shot learning on all the three models. After that we choose the category of the disaster based on the majority voting of the three configurations.\n",
    "\n",
    "### Objectives:\n",
    "- To get familiar with existing pretrained LLMs.\n",
    "- To get hands on experience with fine-tuning existing pretrained LLMs.\n",
    "- To explore different fine-tuning techniques.\n",
    "- To classify disaster categories from tweets.\n",
    "- To leverage pretrained LLMs for improving accuracy of classification.\n",
    "\n",
    "**Dataset link:** https://archive.ics.uci.edu/ml/datasets/Multimodal+Damage+Identification+for+Humanitarian+Computing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ac42d7-00cc-4316-82ac-ab6df48f58fd",
   "metadata": {},
   "source": [
    "### Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "663fea98-e2ae-4483-ba52-b049d5afc824",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "from bs4 import BeautifulSoup\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict, Counter\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from peft import LoraConfig, get_peft_model, TaskType, PromptTuningConfig\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer, \n",
    "    TrainingArguments,\n",
    "    BertConfig,\n",
    "    RobertaConfig,\n",
    "    DistilBertConfig\n",
    ")\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from transformers import DataCollatorWithPadding, logging\n",
    "logging.set_verbosity_error()  # Suppress initialization warnings\n",
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71c36d0e-3448-47b5-938f-0e8579a2a38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3cd68f8-0333-4e81-89ae-fac5acc361b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# Setting the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169802e4-47c4-4f35-8240-6e6cb72e3656",
   "metadata": {},
   "source": [
    "### Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c08749c-a2c1-4677-968a-216ec22ad5d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data directory: /Users/sahmed8/Desktop/llm-project/v2.0/data/multimodal\n",
      "Disaster categories: ['human_damage', 'fires', '.DS_Store', 'damaged_nature', 'flood', 'non_damage', 'damaged_infrastructure']\n"
     ]
    }
   ],
   "source": [
    "BASE_DIR = os.getcwd()\n",
    "DATA_DIR = os.path.join(BASE_DIR, 'data', 'multimodal')\n",
    "\n",
    "print(f\"Data directory: {DATA_DIR}\")\n",
    "print(f\"Disaster categories: {os.listdir(DATA_DIR)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4697ae7d-df82-45f6-b2f2-0e0b0b57b16f",
   "metadata": {},
   "source": [
    "### Create the Dataset\n",
    "- Create the text dataset from different directories.\n",
    "- The dataset used here is a multimodal dataset. However, we used only the text in this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b78b498a-3eff-4e39-a044-f736d57d6dac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the dataset: 5831\n"
     ]
    }
   ],
   "source": [
    "# Initialize a list to store data\n",
    "data = []\n",
    "\n",
    "# Iterate through each disaster class folder\n",
    "for class_name in os.listdir(DATA_DIR):\n",
    "    class_folder = os.path.join(DATA_DIR, class_name)\n",
    "\n",
    "    # Ensure it's a directory\n",
    "    if os.path.isdir(class_folder):\n",
    "        # Go one level deeper into the `text` subfolder\n",
    "        text_folder = os.path.join(class_folder, 'text')\n",
    "\n",
    "        if os.path.exists(text_folder) and os.path.isdir(text_folder):\n",
    "            for file_name in os.listdir(text_folder):\n",
    "                if file_name.endswith('.txt'):\n",
    "                    file_path = os.path.join(text_folder, file_name)\n",
    "\n",
    "                    # Read text content\n",
    "                    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                        text = f.read().strip()\n",
    "\n",
    "                    # Append text with class label\n",
    "                    data.append({'text': text, 'label': class_name})\n",
    "\n",
    "\n",
    "# Print the total number of samples\n",
    "print(f\"Length of the dataset: {len(data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7741db37-f0e5-45dc-98b6-a18e4edb364d",
   "metadata": {},
   "source": [
    "### Statistical Information About the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bbbd907c-664c-4642-8e59-0a60e64019ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How they welcome syrian refugees in Macedonia ...</td>\n",
       "      <td>human_damage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>When the coalition is no different than Assad ...</td>\n",
       "      <td>human_damage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Syrian children are the primary target of Syri...</td>\n",
       "      <td>human_damage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Syrian genocide continues........#assadcrimes ...</td>\n",
       "      <td>human_damage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This is #Yemen| Child BURIED under RUBBLE as 9...</td>\n",
       "      <td>human_damage</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text         label\n",
       "0  How they welcome syrian refugees in Macedonia ...  human_damage\n",
       "1  When the coalition is no different than Assad ...  human_damage\n",
       "2  Syrian children are the primary target of Syri...  human_damage\n",
       "3  Syrian genocide continues........#assadcrimes ...  human_damage\n",
       "4  This is #Yemen| Child BURIED under RUBBLE as 9...  human_damage"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Creating a Pandas dataframe\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display first 5 rows of the dataframe\n",
    "display(df.head())\n",
    "\n",
    "# Get the number of categories\n",
    "num_classes = len(df['label'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ad35514-9507-4086-80a2-bcc4bb78a870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets in each category:\n",
      "===================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "label\n",
       "non_damage                2957\n",
       "damaged_infrastructure    1390\n",
       "damaged_nature             514\n",
       "flood                      384\n",
       "fires                      346\n",
       "human_damage               240\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Number of tweets in each category:\")\n",
    "print(f\"===================================\")\n",
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb99ebd9-5339-47a8-8617-95dcd1a32287",
   "metadata": {},
   "source": [
    "### Text Preprocessing\n",
    "- We used beautiful soap to remove the HTML tags.\n",
    "- Removed special characters except # symbols as the symbol in tweets contain information about certain events.\n",
    "- Converted to lower case and removed the white spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37719f1b-d769-4b9a-a970-00e2ed1c465e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean text\n",
    "def clean_text(txt):\n",
    "    txt = BeautifulSoup(txt, 'html.parser').get_text()\n",
    "    txt = re.sub(r'https?://\\S+', '', txt)\n",
    "    txt = re.sub(r'[^#@A-Za-z0-9 ]+', ' ', txt).lower().strip()\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb8cecd7-f4fa-41ba-849b-7f558bfbef71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the cleaned tweets in a new column of the dataframe\n",
    "df['cleaned'] = df['text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ea3c671-feb6-4f8f-b644-e9accd54c278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data samples after cleaning:\n",
      "=====================================\n",
      "Original Data: How they welcome syrian refugees in Macedonia .......shame on this world to see the suffering of Syrian and doing nothing to help them ..........#syria #syrians #syrie #assadcrimes #isiscrimes #refugees #syriangenocide #genocide #un #unitednations #syrianrefugees #syrianorphans\n",
      "----------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned Data: how they welcome syrian refugees in macedonia  shame on this world to see the suffering of syrian and doing nothing to help them  #syria #syrians #syrie #assadcrimes #isiscrimes #refugees #syriangenocide #genocide #un #unitednations #syrianrefugees #syrianorphans\n",
      "==================================================================================================================================\n",
      "Original Data: When the coalition is no different than Assad and Russia in attacking civilians with White Phosphorus.  This descending hell fire has killed 14 civilians in #Raqqa,  right on the spot.\n",
      "\n",
      "#Coalition_kills_civilians .\n",
      ".\n",
      "#everychildismychild #AssadHolocaust #AssadWarcrimes #EnoughwithAssad #SyriaWarCrimes\n",
      "#AssadMustGo #ChemicalAttack\n",
      "----------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned Data: when the coalition is no different than assad and russia in attacking civilians with white phosphorus   this descending hell fire has killed 14 civilians in #raqqa   right on the spot #coalition kills civilians  #everychildismychild #assadholocaust #assadwarcrimes #enoughwithassad #syriawarcrimes #assadmustgo #chemicalattack\n",
      "==================================================================================================================================\n",
      "Original Data: Syrian children are the primary target of Syrian regime bombardment..........until when the world will continues watching silently and doing nothing to save these innocent children????????#assadcrimes #anger #isiscrimes #syria #syrie #syrians #syrianorphans #syriangenocide #genocide #syrianregimegenocide #syrianrefugees #syrianrevolution #children #childrenofsyria #postcard #postcardfromsyria\n",
      "----------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned Data: syrian children are the primary target of syrian regime bombardment until when the world will continues watching silently and doing nothing to save these innocent children #assadcrimes #anger #isiscrimes #syria #syrie #syrians #syrianorphans #syriangenocide #genocide #syrianregimegenocide #syrianrefugees #syrianrevolution #children #childrenofsyria #postcard #postcardfromsyria\n",
      "==================================================================================================================================\n",
      "Original Data: Syrian genocide continues........#assadcrimes #isiscrimes #suffering #syrians #syrie #syria\n",
      "----------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned Data: syrian genocide continues #assadcrimes #isiscrimes #suffering #syrians #syrie #syria\n",
      "==================================================================================================================================\n",
      "Original Data: This is #Yemen| Child BURIED under RUBBLE as 90% of #Saudi airstrikes attack homes & not military targets.\n",
      "#SaudiCrimes #YemenUnderAttack #YemenCrisis #YemenChildern #UN @UN #HRC30 \n",
      "#إعادة_الأمل #جرائم_العدوان_السعودي\n",
      "#اليمن\n",
      "... https://t.co/qp6LnKpH2j\n",
      "----------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned Data: this is #yemen  child buried under rubble as 90  of #saudi airstrikes attack homes   not military targets #saudicrimes #yemenunderattack #yemencrisis #yemenchildern #un @un #hrc30  #  # #\n",
      "==================================================================================================================================\n",
      "Original Data: 17 children were killed this month alone by Assad regime mainly by air strikes and by dropping explosive barrels. The Syrian regime genocide against Syrian kids continues and the humanity and international community continues watching silently the atrocities of Assad army #syrians #syria #syrie #assadcrimes #isiscrimes #wakeupworld #shameonhumanity #shameonthisworld #pain #refugee #red #redcolorissyriacolor #pain #poor #child #children #childinsyria #childrenofsyria\n",
      "----------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned Data: 17 children were killed this month alone by assad regime mainly by air strikes and by dropping explosive barrels  the syrian regime genocide against syrian kids continues and the humanity and international community continues watching silently the atrocities of assad army #syrians #syria #syrie #assadcrimes #isiscrimes #wakeupworld #shameonhumanity #shameonthisworld #pain #refugee #red #redcolorissyriacolor #pain #poor #child #children #childinsyria #childrenofsyria\n",
      "==================================================================================================================================\n",
      "Original Data: Yemen girls that look like this are being stolen from their mothers and sold into human traffickers for sex. Isis says if their eyes are blue or green they are sold at a premium rate. Crimes agsingts humanity happening right now. #crimesagainsthumanity  #humantrafficking  #savethesechildren\n",
      "----------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned Data: yemen girls that look like this are being stolen from their mothers and sold into human traffickers for sex  isis says if their eyes are blue or green they are sold at a premium rate  crimes agsingts humanity happening right now  #crimesagainsthumanity  #humantrafficking  #savethesechildren\n",
      "==================================================================================================================================\n",
      "Original Data: #lasvegas shooting: Two dead, 24 wounded by gunfire, says #hospital #terrorattack\n",
      "----------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned Data: #lasvegas shooting  two dead  24 wounded by gunfire  says #hospital #terrorattack\n",
      "==================================================================================================================================\n",
      "Original Data: Every day Syrian children are being injured or killed by Assad regime or Isis (daesh) . Humanity and the international community keeps watching silently with no serious action to stop the children massacre. .......just imagine this injured son is your son .....just imagine..........#assadcrimes #shameonhumanity #enfantsdelasyrie #syria #syrie #syrians #syrianrefugee #assadcrimes #isiscrimes #injured #ihateisis #isiscrimes #injuredchildren #injured\n",
      "----------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned Data: every day syrian children are being injured or killed by assad regime or isis  daesh    humanity and the international community keeps watching silently with no serious action to stop the children massacre   just imagine this injured son is your son  just imagine #assadcrimes #shameonhumanity #enfantsdelasyrie #syria #syrie #syrians #syrianrefugee #assadcrimes #isiscrimes #injured #ihateisis #isiscrimes #injuredchildren #injured\n",
      "==================================================================================================================================\n",
      "Original Data: #victimsofwar #reflection #emotionalintelligence\n",
      "----------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned Data: #victimsofwar #reflection #emotionalintelligence\n",
      "==================================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Data samples after cleaning\n",
    "print(\"Data samples after cleaning:\")\n",
    "print(f\"=====================================\")\n",
    "\n",
    "for i in range(10):\n",
    "    print(f\"Original Data: {df.text[i]}\")\n",
    "    print(\"--\"*65)\n",
    "    print(f\"Cleaned Data: {df.cleaned[i]}\")\n",
    "    print(f\"==\"*65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e6342f2d-09c0-4ffd-8f5e-dcbcd08a7475",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>cleaned</th>\n",
       "      <th>enc_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How they welcome syrian refugees in Macedonia ...</td>\n",
       "      <td>human_damage</td>\n",
       "      <td>how they welcome syrian refugees in macedonia ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>When the coalition is no different than Assad ...</td>\n",
       "      <td>human_damage</td>\n",
       "      <td>when the coalition is no different than assad ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Syrian children are the primary target of Syri...</td>\n",
       "      <td>human_damage</td>\n",
       "      <td>syrian children are the primary target of syri...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Syrian genocide continues........#assadcrimes ...</td>\n",
       "      <td>human_damage</td>\n",
       "      <td>syrian genocide continues #assadcrimes #isiscr...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This is #Yemen| Child BURIED under RUBBLE as 9...</td>\n",
       "      <td>human_damage</td>\n",
       "      <td>this is #yemen  child buried under rubble as 9...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text         label  \\\n",
       "0  How they welcome syrian refugees in Macedonia ...  human_damage   \n",
       "1  When the coalition is no different than Assad ...  human_damage   \n",
       "2  Syrian children are the primary target of Syri...  human_damage   \n",
       "3  Syrian genocide continues........#assadcrimes ...  human_damage   \n",
       "4  This is #Yemen| Child BURIED under RUBBLE as 9...  human_damage   \n",
       "\n",
       "                                             cleaned enc_label  \n",
       "0  how they welcome syrian refugees in macedonia ...         5  \n",
       "1  when the coalition is no different than assad ...         5  \n",
       "2  syrian children are the primary target of syri...         5  \n",
       "3  syrian genocide continues #assadcrimes #isiscr...         5  \n",
       "4  this is #yemen  child buried under rubble as 9...         5  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Encode labels as ints\n",
    "pd.set_option('future.no_silent_downcasting', True)\n",
    "df['enc_label'] = df['label'].replace({'non_damage':0, \n",
    "                                       'damaged_infrastructure':1, \n",
    "                                       'damaged_nature':2, \n",
    "                                       'fires':3, \n",
    "                                       'flood':4, \n",
    "                                       'human_damage':5})\n",
    "\n",
    "# Display the updated df\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "156de333-ab1f-46b3-b32e-3e9b612ac755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets in each category:\n",
      "===================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "enc_label\n",
       "0    2957\n",
       "1    1390\n",
       "2     514\n",
       "4     384\n",
       "3     346\n",
       "5     240\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Number of tweets in each category:\")\n",
    "print(f\"===================================\")\n",
    "df['enc_label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15695fbe-6433-48e5-ac06-ebd62d949a76",
   "metadata": {},
   "source": [
    "### Dataset Splitting\n",
    "- Split the dataset into training, validation and test sets.\n",
    "- The ratio is Train (80%), Validation (10%) and Test (10%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "907544bf-3f88-412a-90a8-7f23a7180ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 4664, Val size: 583, Test size: 584\n"
     ]
    }
   ],
   "source": [
    "# Split data\n",
    "train_texts, temp_texts, train_labels, temp_labels = train_test_split(\n",
    "    df['cleaned'], df['enc_label'],\n",
    "    test_size=0.2, stratify=df['enc_label'], random_state=42\n",
    ")\n",
    "\n",
    "val_texts, test_texts, val_labels, test_labels = train_test_split(\n",
    "    temp_texts, temp_labels, test_size=0.5, stratify=temp_labels, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Train size: {len(train_texts)}, Val size: {len(val_texts)}, Test size: {len(test_texts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "923349a4-38a4-48e4-b202-f319e67cd17b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:\n",
      "enc_label\n",
      "0    2365\n",
      "1    1112\n",
      "2     411\n",
      "4     307\n",
      "3     277\n",
      "5     192\n",
      "Name: count, dtype: int64\n",
      "Validation set:\n",
      "enc_label\n",
      "0    296\n",
      "1    139\n",
      "2     51\n",
      "4     38\n",
      "3     35\n",
      "5     24\n",
      "Name: count, dtype: int64\n",
      "Test set:\n",
      "enc_label\n",
      "0    296\n",
      "1    139\n",
      "2     52\n",
      "4     39\n",
      "3     34\n",
      "5     24\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Number of tweets in each category after splitting\n",
    "print(f\"Training set:\")\n",
    "print(train_labels.value_counts())\n",
    "\n",
    "print(f\"Validation set:\")\n",
    "print(val_labels.value_counts())\n",
    "\n",
    "print(f\"Test set:\")\n",
    "print(test_labels.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fde696e-853b-4ab3-a904-e1418fab7073",
   "metadata": {},
   "source": [
    "### Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "949f07d1-9901-441a-94cd-6ad90b8328de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class\n",
    "torch_device = device  # for Trainer\n",
    "\n",
    "class DisasterDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        item = {k:v[idx].to(torch_device) for k,v in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], device=torch_device)\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "45ca8345-4b31-41ac-8adc-c09285bef837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare tokenized datasets\n",
    "def prepare_datasets(model_name, max_length=150):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    def tok(txts):\n",
    "        return tokenizer(txts, padding=True, truncation=True,\n",
    "                         max_length=max_length, return_tensors='pt')\n",
    "    train_enc = tok(train_texts.tolist())\n",
    "    val_enc   = tok(val_texts.tolist())\n",
    "    test_enc  = tok(test_texts.tolist())\n",
    "    \n",
    "    return (\n",
    "        DisasterDataset(train_enc, train_labels.tolist()),\n",
    "        DisasterDataset(val_enc,   val_labels.tolist()),\n",
    "        DisasterDataset(test_enc,  test_labels.tolist()),\n",
    "        tokenizer\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8fb2c0-edb3-4424-bd36-2a5066c4ef98",
   "metadata": {},
   "source": [
    "- Selects number of samples based on the parameter `SHOTS_PER_CLASS` for few-shot learning.\n",
    "- We used the training dataset to select samples for few-shot learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "993eb17b-3c17-4815-8753-39aaa1916aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper for few-shot subsampling\n",
    "def get_few_shot_dataset(full_dataset, shots_per_class):\n",
    "    labels = full_dataset.labels\n",
    "    indices_by_label = defaultdict(list)\n",
    "    \n",
    "    for idx, lab in enumerate(labels):\n",
    "        indices_by_label[lab].append(idx)\n",
    "        \n",
    "    selected = []\n",
    "    \n",
    "    for lab, idxs in indices_by_label.items():\n",
    "        k = min(shots_per_class, len(idxs))\n",
    "        selected += random.sample(idxs, k)\n",
    "        \n",
    "    selected.sort()\n",
    "    \n",
    "    enc = {k: v[selected] for k, v in full_dataset.encodings.items()}\n",
    "    labs = [labels[i] for i in selected]\n",
    "    \n",
    "    return DisasterDataset(enc, labs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd45f125-180b-411c-a5af-913f44e6a017",
   "metadata": {},
   "source": [
    "**Metics function to compute precision, recall and f1 score.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f2a0ca2c-a9d0-449d-b826-bda17ec07997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics function\n",
    "def compute_metrics(eval_pred):\n",
    "    preds, labs = eval_pred\n",
    "    pred_labels = np.argmax(preds, axis=1)\n",
    "    \n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labs, pred_labels, average='weighted', zero_division=0\n",
    "    )\n",
    "    \n",
    "    acc = (pred_labels == labs).mean()\n",
    "    \n",
    "    return {'precision': precision, 'recall': recall, 'f1': f1, 'accuracy': acc}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e59aa44-bae5-4a63-b161-d54f4a7eb4a5",
   "metadata": {},
   "source": [
    "**Hyperparameters to fine-tune the models.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d3e41621-683e-4534-bdc7-4e43c40e9cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters:\n",
    "NUM_EPOCHS = 8\n",
    "\n",
    "# few-shot: number of examples per class\n",
    "SHOTS_PER_CLASS = 20  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c64ce20-8170-4f28-88a1-3e741f5de960",
   "metadata": {},
   "source": [
    "### Standard Fine-Tuning\n",
    "- Updated all layers to better represent the features of disaster classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6589ff62-3757-49c3-9d26-4bd936ce92e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Fine-Tuning\n",
    "def run_standard_finetuning(model_name, tokenizer):\n",
    "    print(f\"\\n=== Standard Fine-Tuning ({model_name}) ===\")\n",
    "    \n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name, \n",
    "        num_labels=num_classes\n",
    "    )\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f'./{model_name}_standard_ft',\n",
    "        num_train_epochs=NUM_EPOCHS,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=32,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_steps=10,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"f1\",\n",
    "        dataloader_pin_memory=False,\n",
    "        report_to=\"none\",\n",
    "        logging_strategy=\"epoch\"\n",
    "    )\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "        data_collator=DataCollatorWithPadding(tokenizer) if tokenizer else None\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "    results = trainer.evaluate(test_dataset)\n",
    "    \n",
    "    preds = trainer.predict(test_dataset)\n",
    "    y_pred = np.argmax(preds.predictions, axis=1)\n",
    "    y_true = preds.label_ids\n",
    "    report = classification_report(y_true, y_pred, output_dict=True)\n",
    "    \n",
    "    return {\n",
    "        'model': model,\n",
    "        'metrics': results,\n",
    "        'classification_report': report,\n",
    "        'num_params': sum(p.numel() for p in model.parameters()),\n",
    "        'trainable_params': sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991bd030-975f-4f2d-ab9b-791567754a6d",
   "metadata": {},
   "source": [
    "### LoRA Fine-Tuning\n",
    "- Fine-Tune the model using Low Rank Adapter approach.\n",
    "- `r` in the `LoraConfig()` represents the rank of LoRA matrices. So, instead of using a full weight matrix $W \\in R^{d_{out} \\times d_{in}}$, LoRA decomposes it into smaller matrices: $W_{lora} = A . B, \\quad A \\in R^{d_{out} \\times r}, \\quad B \\in R^{r \\times d_{in}}$.\n",
    "- We choose to use use `r=8` as it gives reasonable F1 score for the computing resources at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b80d3431-e05b-4cb7-8720-2e73e0a1a4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA Fine-Tuning\n",
    "def run_lora_finetuning(model_name, tokenizer):\n",
    "    print(f\"\\n=== LoRA Fine-Tuning ({model_name}) ===\")\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name, \n",
    "        num_labels=num_classes\n",
    "    )\n",
    "    \n",
    "    # LoRA configuration\n",
    "    target_modules = {\n",
    "        \"bert\": [\"query\", \"value\"],\n",
    "        \"roberta\": [\"query\", \"value\"],\n",
    "        \"distilbert\": [\"q_lin\", \"v_lin\"]\n",
    "    }[model_name.split('-')[0]]\n",
    "    \n",
    "    peft_config = LoraConfig(\n",
    "        task_type=TaskType.SEQ_CLS,\n",
    "        inference_mode=False,\n",
    "        r=8, \n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.1,\n",
    "        target_modules=target_modules\n",
    "    )\n",
    "    \n",
    "    model = get_peft_model(model, peft_config)\n",
    "    model.print_trainable_parameters()\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f'./{model_name}_lora_ft',\n",
    "        num_train_epochs=NUM_EPOCHS,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=32,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_steps=10,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model='f1',\n",
    "        dataloader_pin_memory=False,\n",
    "        report_to=\"none\",\n",
    "        logging_strategy=\"epoch\"\n",
    "    )\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "        data_collator=DataCollatorWithPadding(tokenizer) if tokenizer else None\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "    results = trainer.evaluate(test_dataset)\n",
    "    \n",
    "    preds = trainer.predict(test_dataset)\n",
    "    y_pred = np.argmax(preds.predictions, axis=1)\n",
    "    y_true = preds.label_ids\n",
    "    report = classification_report(y_true, y_pred, output_dict=True)\n",
    "    \n",
    "    return {\n",
    "        'model': model,\n",
    "        'metrics': results,\n",
    "        'classification_report': report,\n",
    "        'num_params': sum(p.numel() for p in model.parameters()),\n",
    "        'trainable_params': sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54702f9c-6ecf-42ba-b325-b7424f286d18",
   "metadata": {},
   "source": [
    "### Prompt Tuning\n",
    "- It trains the model using few-shot learning where the number of examples is chosen based on the value of `SHOTS_PER_CLASS`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0e67522f-9f80-46c3-bf15-c033f5e166a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_prompt_tuning(model_name, tokenizer, train_ds, val_ds, test_ds, shots_per_class=None):\n",
    "    print(f\"\\n=== Prompt Tuning ({model_name}) ===\")\n",
    "    \n",
    "    # If few-shot requested, subsample\n",
    "    if shots_per_class is not None:\n",
    "        print(f\"Using {shots_per_class} shots per class (total ~{shots_per_class * num_classes} samples)\")\n",
    "        train_ds = get_few_shot_dataset(train_ds, shots_per_class)\n",
    "        \n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name, num_labels=num_classes\n",
    "    )\n",
    "    \n",
    "    # Prompt tuning config\n",
    "    peft_cfg = PromptTuningConfig(\n",
    "        task_type=TaskType.SEQ_CLS,\n",
    "        num_virtual_tokens=20,\n",
    "        num_layers=model.config.num_hidden_layers,\n",
    "        token_dim=model.config.hidden_size,\n",
    "        num_attention_heads=model.config.num_attention_heads,\n",
    "        inference_mode=False\n",
    "    )\n",
    "    \n",
    "    model = get_peft_model(model, peft_cfg)\n",
    "    model.print_trainable_parameters()\n",
    "    \n",
    "    args = TrainingArguments(\n",
    "        output_dir=f'./{model_name}_prompt_tuning_fs',\n",
    "        num_train_epochs=NUM_EPOCHS,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=32,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_steps=10,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model='f1',\n",
    "        dataloader_pin_memory=False,\n",
    "        report_to=\"none\",\n",
    "        logging_strategy=\"epoch\"\n",
    "    )\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model, args=args,\n",
    "        train_dataset=train_ds, eval_dataset=val_ds,\n",
    "        compute_metrics=compute_metrics,\n",
    "        data_collator=DataCollatorWithPadding(tokenizer)\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "    \n",
    "    results = trainer.evaluate(test_ds)\n",
    "    preds = trainer.predict(test_ds)\n",
    "    y_pred = np.argmax(preds.predictions, axis=1)\n",
    "    y_true = preds.label_ids\n",
    "    \n",
    "    report = classification_report(\n",
    "        y_true, y_pred, output_dict=True\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'model': model,\n",
    "        'metrics': results, \n",
    "        'classification_report': report,\n",
    "        'num_params': sum(p.numel() for p in model.parameters()),\n",
    "        'trainable_params': sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e61afd0-197c-4c62-a6bb-04cc00fa568b",
   "metadata": {},
   "source": [
    "### Model Training\n",
    "- Train the each of the three models for different fine-tuning approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "98fce387-407d-49ad-9256-725fee5f3bd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Running Experiments for BERT-BASE-UNCASED =====\n",
      "\n",
      "=== Standard Fine-Tuning (bert-base-uncased) ===\n",
      "{'loss': 0.5468, 'grad_norm': 2.456413507461548, 'learning_rate': 4.3771404109589045e-05, 'epoch': 1.0}\n",
      "{'eval_loss': 0.28122493624687195, 'eval_precision': 0.9177878873888076, 'eval_recall': 0.9142367066895368, 'eval_f1': 0.9149854091529339, 'eval_accuracy': 0.9142367066895368, 'eval_runtime': 8.4483, 'eval_samples_per_second': 69.008, 'eval_steps_per_second': 2.249, 'epoch': 1.0}\n",
      "{'loss': 0.2426, 'grad_norm': 0.45614320039749146, 'learning_rate': 3.752140410958904e-05, 'epoch': 2.0}\n",
      "{'eval_loss': 0.34063026309013367, 'eval_precision': 0.9100120647702898, 'eval_recall': 0.9056603773584906, 'eval_f1': 0.9070894213768718, 'eval_accuracy': 0.9056603773584906, 'eval_runtime': 8.3722, 'eval_samples_per_second': 69.635, 'eval_steps_per_second': 2.269, 'epoch': 2.0}\n",
      "{'loss': 0.1651, 'grad_norm': 6.342898368835449, 'learning_rate': 3.127140410958904e-05, 'epoch': 3.0}\n",
      "{'eval_loss': 0.37090447545051575, 'eval_precision': 0.9117422567077764, 'eval_recall': 0.9090909090909091, 'eval_f1': 0.9087013735446129, 'eval_accuracy': 0.9090909090909091, 'eval_runtime': 8.3482, 'eval_samples_per_second': 69.836, 'eval_steps_per_second': 2.276, 'epoch': 3.0}\n",
      "{'loss': 0.1048, 'grad_norm': 2.3253486156463623, 'learning_rate': 2.5021404109589043e-05, 'epoch': 4.0}\n",
      "{'eval_loss': 0.5229765176773071, 'eval_precision': 0.9008816810992215, 'eval_recall': 0.8953687821612349, 'eval_f1': 0.8961432905299171, 'eval_accuracy': 0.8953687821612349, 'eval_runtime': 8.311, 'eval_samples_per_second': 70.148, 'eval_steps_per_second': 2.286, 'epoch': 4.0}\n",
      "{'loss': 0.0741, 'grad_norm': 43.529396057128906, 'learning_rate': 1.877140410958904e-05, 'epoch': 5.0}\n",
      "{'eval_loss': 0.5156310796737671, 'eval_precision': 0.8983721358087955, 'eval_recall': 0.8936535162950258, 'eval_f1': 0.8954827017896367, 'eval_accuracy': 0.8936535162950258, 'eval_runtime': 8.3558, 'eval_samples_per_second': 69.772, 'eval_steps_per_second': 2.274, 'epoch': 5.0}\n",
      "{'loss': 0.0446, 'grad_norm': 0.25379854440689087, 'learning_rate': 1.252140410958904e-05, 'epoch': 6.0}\n",
      "{'eval_loss': 0.6077311635017395, 'eval_precision': 0.902100988563496, 'eval_recall': 0.8987993138936535, 'eval_f1': 0.898347044513151, 'eval_accuracy': 0.8987993138936535, 'eval_runtime': 8.3445, 'eval_samples_per_second': 69.866, 'eval_steps_per_second': 2.277, 'epoch': 6.0}\n",
      "{'loss': 0.0353, 'grad_norm': 0.04255945235490799, 'learning_rate': 6.2714041095890405e-06, 'epoch': 7.0}\n",
      "{'eval_loss': 0.5519087314605713, 'eval_precision': 0.9073402727340304, 'eval_recall': 0.9056603773584906, 'eval_f1': 0.9061081441743517, 'eval_accuracy': 0.9056603773584906, 'eval_runtime': 8.3581, 'eval_samples_per_second': 69.753, 'eval_steps_per_second': 2.273, 'epoch': 7.0}\n",
      "{'loss': 0.0242, 'grad_norm': 0.013539624400436878, 'learning_rate': 2.1404109589041097e-08, 'epoch': 8.0}\n",
      "{'eval_loss': 0.5882787108421326, 'eval_precision': 0.9013394149001656, 'eval_recall': 0.8987993138936535, 'eval_f1': 0.8986141355865533, 'eval_accuracy': 0.8987993138936535, 'eval_runtime': 8.3531, 'eval_samples_per_second': 69.794, 'eval_steps_per_second': 2.275, 'epoch': 8.0}\n",
      "{'train_runtime': 1978.5577, 'train_samples_per_second': 18.858, 'train_steps_per_second': 1.181, 'train_loss': 0.1546996772697527, 'epoch': 8.0}\n",
      "{'eval_loss': 0.3396497070789337, 'eval_precision': 0.8995960018773717, 'eval_recall': 0.8972602739726028, 'eval_f1': 0.8956805117803074, 'eval_accuracy': 0.8972602739726028, 'eval_runtime': 8.4616, 'eval_samples_per_second': 69.018, 'eval_steps_per_second': 2.245, 'epoch': 8.0}\n",
      "\n",
      "=== LoRA Fine-Tuning (bert-base-uncased) ===\n",
      "trainable params: 299,526 || all params: 109,786,380 || trainable%: 0.2728\n",
      "{'loss': 1.301, 'grad_norm': 6.074240207672119, 'learning_rate': 4.3771404109589045e-05, 'epoch': 1.0}\n",
      "{'eval_loss': 0.9489328265190125, 'eval_precision': 0.531212113809161, 'eval_recall': 0.6689536878216124, 'eval_f1': 0.5860154543315685, 'eval_accuracy': 0.6689536878216124, 'eval_runtime': 8.8448, 'eval_samples_per_second': 65.915, 'eval_steps_per_second': 2.148, 'epoch': 1.0}\n",
      "{'loss': 0.8457, 'grad_norm': 4.790751934051514, 'learning_rate': 3.752140410958904e-05, 'epoch': 2.0}\n",
      "{'eval_loss': 0.7644983530044556, 'eval_precision': 0.7135826967475587, 'eval_recall': 0.7049742710120068, 'eval_f1': 0.6383190852040824, 'eval_accuracy': 0.7049742710120068, 'eval_runtime': 8.8651, 'eval_samples_per_second': 65.764, 'eval_steps_per_second': 2.143, 'epoch': 2.0}\n",
      "{'loss': 0.6979, 'grad_norm': 5.72885799407959, 'learning_rate': 3.127140410958904e-05, 'epoch': 3.0}\n",
      "{'eval_loss': 0.6672216653823853, 'eval_precision': 0.6827906763048273, 'eval_recall': 0.7427101200686106, 'eval_f1': 0.6976501330157965, 'eval_accuracy': 0.7427101200686106, 'eval_runtime': 8.8529, 'eval_samples_per_second': 65.854, 'eval_steps_per_second': 2.146, 'epoch': 3.0}\n",
      "{'loss': 0.598, 'grad_norm': 7.127647399902344, 'learning_rate': 2.5021404109589043e-05, 'epoch': 4.0}\n",
      "{'eval_loss': 0.6039940714836121, 'eval_precision': 0.8077535974218225, 'eval_recall': 0.7650085763293311, 'eval_f1': 0.7279030223838553, 'eval_accuracy': 0.7650085763293311, 'eval_runtime': 8.8556, 'eval_samples_per_second': 65.834, 'eval_steps_per_second': 2.146, 'epoch': 4.0}\n",
      "{'loss': 0.5394, 'grad_norm': 3.7027854919433594, 'learning_rate': 1.877140410958904e-05, 'epoch': 5.0}\n",
      "{'eval_loss': 0.5737729072570801, 'eval_precision': 0.7583803957016033, 'eval_recall': 0.7804459691252144, 'eval_f1': 0.7614744418976269, 'eval_accuracy': 0.7804459691252144, 'eval_runtime': 8.9454, 'eval_samples_per_second': 65.173, 'eval_steps_per_second': 2.124, 'epoch': 5.0}\n",
      "{'loss': 0.5176, 'grad_norm': 2.4925014972686768, 'learning_rate': 1.252140410958904e-05, 'epoch': 6.0}\n",
      "{'eval_loss': 0.550191342830658, 'eval_precision': 0.7912830359866205, 'eval_recall': 0.8010291595197255, 'eval_f1': 0.78516814531933, 'eval_accuracy': 0.8010291595197255, 'eval_runtime': 8.842, 'eval_samples_per_second': 65.936, 'eval_steps_per_second': 2.149, 'epoch': 6.0}\n",
      "{'loss': 0.4995, 'grad_norm': 7.670921325683594, 'learning_rate': 6.2714041095890405e-06, 'epoch': 7.0}\n",
      "{'eval_loss': 0.5357221961021423, 'eval_precision': 0.8050702994684776, 'eval_recall': 0.8061749571183533, 'eval_f1': 0.7958421864168387, 'eval_accuracy': 0.8061749571183533, 'eval_runtime': 8.867, 'eval_samples_per_second': 65.749, 'eval_steps_per_second': 2.143, 'epoch': 7.0}\n",
      "{'loss': 0.4822, 'grad_norm': 4.810647964477539, 'learning_rate': 2.1404109589041097e-08, 'epoch': 8.0}\n",
      "{'eval_loss': 0.5298964381217957, 'eval_precision': 0.8135165008113961, 'eval_recall': 0.8130360205831904, 'eval_f1': 0.8023221600077564, 'eval_accuracy': 0.8130360205831904, 'eval_runtime': 8.8793, 'eval_samples_per_second': 65.658, 'eval_steps_per_second': 2.14, 'epoch': 8.0}\n",
      "{'train_runtime': 1481.5232, 'train_samples_per_second': 25.185, 'train_steps_per_second': 1.577, 'train_loss': 0.6851666267604044, 'epoch': 8.0}\n",
      "{'eval_loss': 0.5032731890678406, 'eval_precision': 0.8294929708078136, 'eval_recall': 0.839041095890411, 'eval_f1': 0.8226963062519868, 'eval_accuracy': 0.839041095890411, 'eval_runtime': 8.919, 'eval_samples_per_second': 65.478, 'eval_steps_per_second': 2.13, 'epoch': 8.0}\n",
      "\n",
      "=== Prompt Tuning (bert-base-uncased) ===\n",
      "Using 20 shots per class (total ~120 samples)\n",
      "trainable params: 15,360 || all params: 109,502,214 || trainable%: 0.0140\n",
      "{'loss': 1.8005, 'grad_norm': 0.06797774881124496, 'learning_rate': 4.453125e-05, 'epoch': 1.0}\n",
      "{'eval_loss': 1.782616376876831, 'eval_precision': 0.35916079029286574, 'eval_recall': 0.2281303602058319, 'eval_f1': 0.10199608152753047, 'eval_accuracy': 0.2281303602058319, 'eval_runtime': 9.4781, 'eval_samples_per_second': 61.51, 'eval_steps_per_second': 2.005, 'epoch': 1.0}\n",
      "{'loss': 1.7986, 'grad_norm': 0.042337898164987564, 'learning_rate': 3.828125e-05, 'epoch': 2.0}\n",
      "{'eval_loss': 1.782604455947876, 'eval_precision': 0.35916079029286574, 'eval_recall': 0.2281303602058319, 'eval_f1': 0.10199608152753047, 'eval_accuracy': 0.2281303602058319, 'eval_runtime': 9.43, 'eval_samples_per_second': 61.824, 'eval_steps_per_second': 2.015, 'epoch': 2.0}\n",
      "{'loss': 1.8074, 'grad_norm': 0.052485015243291855, 'learning_rate': 3.203125e-05, 'epoch': 3.0}\n",
      "{'eval_loss': 1.7825844287872314, 'eval_precision': 0.35916079029286574, 'eval_recall': 0.2281303602058319, 'eval_f1': 0.10199608152753047, 'eval_accuracy': 0.2281303602058319, 'eval_runtime': 9.5642, 'eval_samples_per_second': 60.956, 'eval_steps_per_second': 1.987, 'epoch': 3.0}\n",
      "{'loss': 1.8052, 'grad_norm': 0.04914303496479988, 'learning_rate': 2.578125e-05, 'epoch': 4.0}\n",
      "{'eval_loss': 1.7825731039047241, 'eval_precision': 0.35916079029286574, 'eval_recall': 0.2281303602058319, 'eval_f1': 0.10199608152753047, 'eval_accuracy': 0.2281303602058319, 'eval_runtime': 9.4291, 'eval_samples_per_second': 61.83, 'eval_steps_per_second': 2.015, 'epoch': 4.0}\n",
      "{'loss': 1.7991, 'grad_norm': 0.05768711119890213, 'learning_rate': 1.953125e-05, 'epoch': 5.0}\n",
      "{'eval_loss': 1.7825523614883423, 'eval_precision': 0.35916079029286574, 'eval_recall': 0.2281303602058319, 'eval_f1': 0.10199608152753047, 'eval_accuracy': 0.2281303602058319, 'eval_runtime': 9.4551, 'eval_samples_per_second': 61.66, 'eval_steps_per_second': 2.01, 'epoch': 5.0}\n",
      "{'loss': 1.8072, 'grad_norm': 0.07004931569099426, 'learning_rate': 1.3281250000000001e-05, 'epoch': 6.0}\n",
      "{'eval_loss': 1.7825462818145752, 'eval_precision': 0.35916079029286574, 'eval_recall': 0.2281303602058319, 'eval_f1': 0.10199608152753047, 'eval_accuracy': 0.2281303602058319, 'eval_runtime': 9.3766, 'eval_samples_per_second': 62.176, 'eval_steps_per_second': 2.026, 'epoch': 6.0}\n",
      "{'loss': 1.8037, 'grad_norm': 0.05642283707857132, 'learning_rate': 7.031250000000001e-06, 'epoch': 7.0}\n",
      "{'eval_loss': 1.7825443744659424, 'eval_precision': 0.35916079029286574, 'eval_recall': 0.2281303602058319, 'eval_f1': 0.10199608152753047, 'eval_accuracy': 0.2281303602058319, 'eval_runtime': 9.3714, 'eval_samples_per_second': 62.211, 'eval_steps_per_second': 2.027, 'epoch': 7.0}\n",
      "{'loss': 1.8092, 'grad_norm': 0.09286646544933319, 'learning_rate': 7.8125e-07, 'epoch': 8.0}\n",
      "{'eval_loss': 1.7825450897216797, 'eval_precision': 0.35916079029286574, 'eval_recall': 0.2281303602058319, 'eval_f1': 0.10199608152753047, 'eval_accuracy': 0.2281303602058319, 'eval_runtime': 9.3664, 'eval_samples_per_second': 62.244, 'eval_steps_per_second': 2.029, 'epoch': 8.0}\n",
      "{'train_runtime': 115.1759, 'train_samples_per_second': 8.335, 'train_steps_per_second': 0.556, 'train_loss': 1.8038495182991028, 'epoch': 8.0}\n",
      "{'eval_loss': 1.775743007659912, 'eval_precision': 0.3999666442914087, 'eval_recall': 0.2345890410958904, 'eval_f1': 0.1032450889061967, 'eval_accuracy': 0.2345890410958904, 'eval_runtime': 9.5129, 'eval_samples_per_second': 61.39, 'eval_steps_per_second': 1.997, 'epoch': 8.0}\n",
      "\n",
      "===== Running Experiments for ROBERTA-BASE =====\n",
      "\n",
      "=== Standard Fine-Tuning (roberta-base) ===\n",
      "{'loss': 0.5533, 'grad_norm': 3.656344175338745, 'learning_rate': 4.3771404109589045e-05, 'epoch': 1.0}\n",
      "{'eval_loss': 0.30149927735328674, 'eval_precision': 0.906628099499228, 'eval_recall': 0.9056603773584906, 'eval_f1': 0.9050974557056988, 'eval_accuracy': 0.9056603773584906, 'eval_runtime': 8.2294, 'eval_samples_per_second': 70.843, 'eval_steps_per_second': 2.309, 'epoch': 1.0}\n",
      "{'loss': 0.2944, 'grad_norm': 9.537835121154785, 'learning_rate': 3.752140410958904e-05, 'epoch': 2.0}\n",
      "{'eval_loss': 0.35265377163887024, 'eval_precision': 0.9038981593286336, 'eval_recall': 0.902229845626072, 'eval_f1': 0.9021502980575244, 'eval_accuracy': 0.902229845626072, 'eval_runtime': 8.1936, 'eval_samples_per_second': 71.153, 'eval_steps_per_second': 2.319, 'epoch': 2.0}\n",
      "{'loss': 0.2491, 'grad_norm': 7.739614486694336, 'learning_rate': 3.127140410958904e-05, 'epoch': 3.0}\n",
      "{'eval_loss': 0.39885053038597107, 'eval_precision': 0.9007122953613945, 'eval_recall': 0.8970840480274442, 'eval_f1': 0.8975713394036349, 'eval_accuracy': 0.8970840480274442, 'eval_runtime': 8.214, 'eval_samples_per_second': 70.977, 'eval_steps_per_second': 2.313, 'epoch': 3.0}\n",
      "{'loss': 0.1823, 'grad_norm': 15.238977432250977, 'learning_rate': 2.5021404109589043e-05, 'epoch': 4.0}\n",
      "{'eval_loss': 0.467607706785202, 'eval_precision': 0.9026053523247058, 'eval_recall': 0.8987993138936535, 'eval_f1': 0.8991131314990349, 'eval_accuracy': 0.8987993138936535, 'eval_runtime': 8.1979, 'eval_samples_per_second': 71.116, 'eval_steps_per_second': 2.318, 'epoch': 4.0}\n",
      "{'loss': 0.1435, 'grad_norm': 20.485246658325195, 'learning_rate': 1.877140410958904e-05, 'epoch': 5.0}\n",
      "{'eval_loss': 0.4748186469078064, 'eval_precision': 0.9072194745248336, 'eval_recall': 0.9005145797598628, 'eval_f1': 0.90283995735951, 'eval_accuracy': 0.9005145797598628, 'eval_runtime': 8.2812, 'eval_samples_per_second': 70.401, 'eval_steps_per_second': 2.294, 'epoch': 5.0}\n",
      "{'loss': 0.1044, 'grad_norm': 29.57876205444336, 'learning_rate': 1.252140410958904e-05, 'epoch': 6.0}\n",
      "{'eval_loss': 0.49556171894073486, 'eval_precision': 0.9086262859645817, 'eval_recall': 0.902229845626072, 'eval_f1': 0.9037427453339453, 'eval_accuracy': 0.902229845626072, 'eval_runtime': 8.1949, 'eval_samples_per_second': 71.142, 'eval_steps_per_second': 2.319, 'epoch': 6.0}\n",
      "{'loss': 0.0698, 'grad_norm': 16.31374740600586, 'learning_rate': 6.2714041095890405e-06, 'epoch': 7.0}\n",
      "{'eval_loss': 0.5467836260795593, 'eval_precision': 0.8979254618374674, 'eval_recall': 0.8970840480274442, 'eval_f1': 0.8973796373423925, 'eval_accuracy': 0.8970840480274442, 'eval_runtime': 8.1942, 'eval_samples_per_second': 71.148, 'eval_steps_per_second': 2.319, 'epoch': 7.0}\n",
      "{'loss': 0.0514, 'grad_norm': 0.033061590045690536, 'learning_rate': 2.1404109589041097e-08, 'epoch': 8.0}\n",
      "{'eval_loss': 0.5867336988449097, 'eval_precision': 0.8997111870940981, 'eval_recall': 0.8987993138936535, 'eval_f1': 0.8987681885317377, 'eval_accuracy': 0.8987993138936535, 'eval_runtime': 8.2133, 'eval_samples_per_second': 70.983, 'eval_steps_per_second': 2.313, 'epoch': 8.0}\n",
      "{'train_runtime': 1986.8641, 'train_samples_per_second': 18.779, 'train_steps_per_second': 1.176, 'train_loss': 0.20604357735751427, 'epoch': 8.0}\n",
      "{'eval_loss': 0.3510024845600128, 'eval_precision': 0.8964056504756274, 'eval_recall': 0.8955479452054794, 'eval_f1': 0.8938202493138148, 'eval_accuracy': 0.8955479452054794, 'eval_runtime': 8.4489, 'eval_samples_per_second': 69.121, 'eval_steps_per_second': 2.249, 'epoch': 8.0}\n",
      "\n",
      "=== LoRA Fine-Tuning (roberta-base) ===\n",
      "trainable params: 890,118 || all params: 125,540,364 || trainable%: 0.7090\n",
      "{'loss': 0.9803, 'grad_norm': 3.8503124713897705, 'learning_rate': 4.3771404109589045e-05, 'epoch': 1.0}\n",
      "{'eval_loss': 0.3879490792751312, 'eval_precision': 0.8713017836114925, 'eval_recall': 0.8713550600343053, 'eval_f1': 0.8687076390049895, 'eval_accuracy': 0.8713550600343053, 'eval_runtime': 8.7414, 'eval_samples_per_second': 66.694, 'eval_steps_per_second': 2.174, 'epoch': 1.0}\n",
      "{'loss': 0.3824, 'grad_norm': 10.52553653717041, 'learning_rate': 3.752140410958904e-05, 'epoch': 2.0}\n",
      "{'eval_loss': 0.3203887641429901, 'eval_precision': 0.8847510963226317, 'eval_recall': 0.8850771869639794, 'eval_f1': 0.8836794742023024, 'eval_accuracy': 0.8850771869639794, 'eval_runtime': 8.7254, 'eval_samples_per_second': 66.816, 'eval_steps_per_second': 2.178, 'epoch': 2.0}\n",
      "{'loss': 0.3308, 'grad_norm': 4.886785507202148, 'learning_rate': 3.127140410958904e-05, 'epoch': 3.0}\n",
      "{'eval_loss': 0.3061728775501251, 'eval_precision': 0.901542169308038, 'eval_recall': 0.902229845626072, 'eval_f1': 0.9015752220926027, 'eval_accuracy': 0.902229845626072, 'eval_runtime': 8.6805, 'eval_samples_per_second': 67.162, 'eval_steps_per_second': 2.189, 'epoch': 3.0}\n",
      "{'loss': 0.299, 'grad_norm': 10.412884712219238, 'learning_rate': 2.5021404109589043e-05, 'epoch': 4.0}\n",
      "{'eval_loss': 0.30402374267578125, 'eval_precision': 0.8993711411070285, 'eval_recall': 0.8987993138936535, 'eval_f1': 0.8986641955509251, 'eval_accuracy': 0.8987993138936535, 'eval_runtime': 8.7181, 'eval_samples_per_second': 66.872, 'eval_steps_per_second': 2.179, 'epoch': 4.0}\n",
      "{'loss': 0.2783, 'grad_norm': 7.844992160797119, 'learning_rate': 1.877140410958904e-05, 'epoch': 5.0}\n",
      "{'eval_loss': 0.32153183221817017, 'eval_precision': 0.9015399088823592, 'eval_recall': 0.8970840480274442, 'eval_f1': 0.8982782867207085, 'eval_accuracy': 0.8970840480274442, 'eval_runtime': 8.6918, 'eval_samples_per_second': 67.075, 'eval_steps_per_second': 2.186, 'epoch': 5.0}\n",
      "{'loss': 0.2686, 'grad_norm': 1.705989122390747, 'learning_rate': 1.252140410958904e-05, 'epoch': 6.0}\n",
      "{'eval_loss': 0.3129723072052002, 'eval_precision': 0.9070664030974227, 'eval_recall': 0.9056603773584906, 'eval_f1': 0.9058038517418413, 'eval_accuracy': 0.9056603773584906, 'eval_runtime': 8.6963, 'eval_samples_per_second': 67.04, 'eval_steps_per_second': 2.185, 'epoch': 6.0}\n",
      "{'loss': 0.26, 'grad_norm': 9.47765827178955, 'learning_rate': 6.2714041095890405e-06, 'epoch': 7.0}\n",
      "{'eval_loss': 0.30894073843955994, 'eval_precision': 0.9009518398028843, 'eval_recall': 0.9005145797598628, 'eval_f1': 0.9005128875695446, 'eval_accuracy': 0.9005145797598628, 'eval_runtime': 8.7321, 'eval_samples_per_second': 66.765, 'eval_steps_per_second': 2.176, 'epoch': 7.0}\n",
      "{'loss': 0.2487, 'grad_norm': 7.10444450378418, 'learning_rate': 2.1404109589041097e-08, 'epoch': 8.0}\n",
      "{'eval_loss': 0.30764085054397583, 'eval_precision': 0.9024260931538693, 'eval_recall': 0.902229845626072, 'eval_f1': 0.9021284989964696, 'eval_accuracy': 0.902229845626072, 'eval_runtime': 8.7247, 'eval_samples_per_second': 66.822, 'eval_steps_per_second': 2.178, 'epoch': 8.0}\n",
      "{'train_runtime': 1464.0295, 'train_samples_per_second': 25.486, 'train_steps_per_second': 1.596, 'train_loss': 0.3810159670163507, 'epoch': 8.0}\n",
      "{'eval_loss': 0.32443729043006897, 'eval_precision': 0.9032144750873125, 'eval_recall': 0.9006849315068494, 'eval_f1': 0.8991378802975029, 'eval_accuracy': 0.9006849315068494, 'eval_runtime': 8.7674, 'eval_samples_per_second': 66.61, 'eval_steps_per_second': 2.167, 'epoch': 8.0}\n",
      "\n",
      "=== Prompt Tuning (roberta-base) ===\n",
      "Using 20 shots per class (total ~120 samples)\n",
      "trainable params: 15,360 || all params: 124,665,606 || trainable%: 0.0123\n",
      "{'loss': 1.8032, 'grad_norm': 0.10517629235982895, 'learning_rate': 4.453125e-05, 'epoch': 1.0}\n",
      "{'eval_loss': 1.8644349575042725, 'eval_precision': 0.0624249170869431, 'eval_recall': 0.2281303602058319, 'eval_f1': 0.09795268464186083, 'eval_accuracy': 0.2281303602058319, 'eval_runtime': 9.3302, 'eval_samples_per_second': 62.486, 'eval_steps_per_second': 2.036, 'epoch': 1.0}\n",
      "{'loss': 1.8072, 'grad_norm': 0.26023098826408386, 'learning_rate': 3.828125e-05, 'epoch': 2.0}\n",
      "{'eval_loss': 1.8644241094589233, 'eval_precision': 0.0624249170869431, 'eval_recall': 0.2281303602058319, 'eval_f1': 0.09795268464186083, 'eval_accuracy': 0.2281303602058319, 'eval_runtime': 9.3062, 'eval_samples_per_second': 62.646, 'eval_steps_per_second': 2.042, 'epoch': 2.0}\n",
      "{'loss': 1.7881, 'grad_norm': 0.061708707362413406, 'learning_rate': 3.203125e-05, 'epoch': 3.0}\n",
      "{'eval_loss': 1.8643989562988281, 'eval_precision': 0.0624249170869431, 'eval_recall': 0.2281303602058319, 'eval_f1': 0.09795268464186083, 'eval_accuracy': 0.2281303602058319, 'eval_runtime': 9.2938, 'eval_samples_per_second': 62.73, 'eval_steps_per_second': 2.044, 'epoch': 3.0}\n",
      "{'loss': 1.8016, 'grad_norm': 0.11420543491840363, 'learning_rate': 2.578125e-05, 'epoch': 4.0}\n",
      "{'eval_loss': 1.8643903732299805, 'eval_precision': 0.0624249170869431, 'eval_recall': 0.2281303602058319, 'eval_f1': 0.09795268464186083, 'eval_accuracy': 0.2281303602058319, 'eval_runtime': 9.2935, 'eval_samples_per_second': 62.732, 'eval_steps_per_second': 2.044, 'epoch': 4.0}\n",
      "{'loss': 1.796, 'grad_norm': 0.07911859452724457, 'learning_rate': 1.953125e-05, 'epoch': 5.0}\n",
      "{'eval_loss': 1.8643851280212402, 'eval_precision': 0.0624249170869431, 'eval_recall': 0.2281303602058319, 'eval_f1': 0.09795268464186083, 'eval_accuracy': 0.2281303602058319, 'eval_runtime': 9.2738, 'eval_samples_per_second': 62.865, 'eval_steps_per_second': 2.049, 'epoch': 5.0}\n",
      "{'loss': 1.8012, 'grad_norm': 0.11560432612895966, 'learning_rate': 1.3281250000000001e-05, 'epoch': 6.0}\n",
      "{'eval_loss': 1.8643854856491089, 'eval_precision': 0.0624249170869431, 'eval_recall': 0.2281303602058319, 'eval_f1': 0.09795268464186083, 'eval_accuracy': 0.2281303602058319, 'eval_runtime': 9.2715, 'eval_samples_per_second': 62.881, 'eval_steps_per_second': 2.049, 'epoch': 6.0}\n",
      "{'loss': 1.8177, 'grad_norm': 0.16303624212741852, 'learning_rate': 7.031250000000001e-06, 'epoch': 7.0}\n",
      "{'eval_loss': 1.864384651184082, 'eval_precision': 0.0624249170869431, 'eval_recall': 0.2281303602058319, 'eval_f1': 0.09795268464186083, 'eval_accuracy': 0.2281303602058319, 'eval_runtime': 9.301, 'eval_samples_per_second': 62.682, 'eval_steps_per_second': 2.043, 'epoch': 7.0}\n",
      "{'loss': 1.7978, 'grad_norm': 0.11134713888168335, 'learning_rate': 7.8125e-07, 'epoch': 8.0}\n",
      "{'eval_loss': 1.8643851280212402, 'eval_precision': 0.0624249170869431, 'eval_recall': 0.2281303602058319, 'eval_f1': 0.09795268464186083, 'eval_accuracy': 0.2281303602058319, 'eval_runtime': 9.2871, 'eval_samples_per_second': 62.776, 'eval_steps_per_second': 2.046, 'epoch': 8.0}\n",
      "{'train_runtime': 114.2408, 'train_samples_per_second': 8.403, 'train_steps_per_second': 0.56, 'train_loss': 1.8015860617160797, 'epoch': 8.0}\n",
      "{'eval_loss': 1.8691306114196777, 'eval_precision': 0.055694905989542756, 'eval_recall': 0.2071917808219178, 'eval_f1': 0.08776922548078832, 'eval_accuracy': 0.2071917808219178, 'eval_runtime': 9.351, 'eval_samples_per_second': 62.453, 'eval_steps_per_second': 2.032, 'epoch': 8.0}\n",
      "\n",
      "===== Running Experiments for DISTILBERT-BASE-UNCASED =====\n",
      "\n",
      "=== Standard Fine-Tuning (distilbert-base-uncased) ===\n",
      "{'loss': 0.4577, 'grad_norm': 1.4589998722076416, 'learning_rate': 4.3771404109589045e-05, 'epoch': 1.0}\n",
      "{'eval_loss': 0.2827325463294983, 'eval_precision': 0.9137114939753284, 'eval_recall': 0.9090909090909091, 'eval_f1': 0.9099029677971792, 'eval_accuracy': 0.9090909090909091, 'eval_runtime': 4.4344, 'eval_samples_per_second': 131.471, 'eval_steps_per_second': 4.285, 'epoch': 1.0}\n",
      "{'loss': 0.2113, 'grad_norm': 0.7418146729469299, 'learning_rate': 3.752140410958904e-05, 'epoch': 2.0}\n",
      "{'eval_loss': 0.3071073293685913, 'eval_precision': 0.9216870821554396, 'eval_recall': 0.9193825042881647, 'eval_f1': 0.9199010389811205, 'eval_accuracy': 0.9193825042881647, 'eval_runtime': 4.452, 'eval_samples_per_second': 130.953, 'eval_steps_per_second': 4.268, 'epoch': 2.0}\n",
      "{'loss': 0.145, 'grad_norm': 2.008394718170166, 'learning_rate': 3.127140410958904e-05, 'epoch': 3.0}\n",
      "{'eval_loss': 0.3825613558292389, 'eval_precision': 0.9119096875304462, 'eval_recall': 0.9108061749571184, 'eval_f1': 0.9100542383785808, 'eval_accuracy': 0.9108061749571184, 'eval_runtime': 4.4642, 'eval_samples_per_second': 130.594, 'eval_steps_per_second': 4.256, 'epoch': 3.0}\n",
      "{'loss': 0.0918, 'grad_norm': 2.320007085800171, 'learning_rate': 2.5021404109589043e-05, 'epoch': 4.0}\n",
      "{'eval_loss': 0.43530339002609253, 'eval_precision': 0.9086276767107566, 'eval_recall': 0.9073756432246999, 'eval_f1': 0.9067048036803156, 'eval_accuracy': 0.9073756432246999, 'eval_runtime': 4.4389, 'eval_samples_per_second': 131.34, 'eval_steps_per_second': 4.28, 'epoch': 4.0}\n",
      "{'loss': 0.056, 'grad_norm': 20.35397720336914, 'learning_rate': 1.877140410958904e-05, 'epoch': 5.0}\n",
      "{'eval_loss': 0.46439534425735474, 'eval_precision': 0.9107933929675233, 'eval_recall': 0.9090909090909091, 'eval_f1': 0.9097573503238282, 'eval_accuracy': 0.9090909090909091, 'eval_runtime': 4.4909, 'eval_samples_per_second': 129.819, 'eval_steps_per_second': 4.231, 'epoch': 5.0}\n",
      "{'loss': 0.0392, 'grad_norm': 0.05135001242160797, 'learning_rate': 1.252140410958904e-05, 'epoch': 6.0}\n",
      "{'eval_loss': 0.5100699663162231, 'eval_precision': 0.9098737062315025, 'eval_recall': 0.9090909090909091, 'eval_f1': 0.9092148678202995, 'eval_accuracy': 0.9090909090909091, 'eval_runtime': 4.8526, 'eval_samples_per_second': 120.141, 'eval_steps_per_second': 3.915, 'epoch': 6.0}\n",
      "{'loss': 0.0262, 'grad_norm': 0.17167477309703827, 'learning_rate': 6.2714041095890405e-06, 'epoch': 7.0}\n",
      "{'eval_loss': 0.5252556800842285, 'eval_precision': 0.9110610890909898, 'eval_recall': 0.9108061749571184, 'eval_f1': 0.9107251517137459, 'eval_accuracy': 0.9108061749571184, 'eval_runtime': 4.7494, 'eval_samples_per_second': 122.751, 'eval_steps_per_second': 4.0, 'epoch': 7.0}\n",
      "{'loss': 0.0194, 'grad_norm': 0.03618905320763588, 'learning_rate': 2.1404109589041097e-08, 'epoch': 8.0}\n",
      "{'eval_loss': 0.5348260402679443, 'eval_precision': 0.9057401669204012, 'eval_recall': 0.9056603773584906, 'eval_f1': 0.9054384642746341, 'eval_accuracy': 0.9056603773584906, 'eval_runtime': 4.9512, 'eval_samples_per_second': 117.749, 'eval_steps_per_second': 3.837, 'epoch': 8.0}\n",
      "{'train_runtime': 1037.3843, 'train_samples_per_second': 35.967, 'train_steps_per_second': 2.252, 'train_loss': 0.13082877842530813, 'epoch': 8.0}\n",
      "{'eval_loss': 0.3186492323875427, 'eval_precision': 0.9063508875588354, 'eval_recall': 0.9058219178082192, 'eval_f1': 0.9053963871420891, 'eval_accuracy': 0.9058219178082192, 'eval_runtime': 4.9226, 'eval_samples_per_second': 118.637, 'eval_steps_per_second': 3.86, 'epoch': 8.0}\n",
      "\n",
      "=== LoRA Fine-Tuning (distilbert-base-uncased) ===\n",
      "trainable params: 742,662 || all params: 67,700,748 || trainable%: 1.0970\n",
      "{'loss': 0.9612, 'grad_norm': 3.1570050716400146, 'learning_rate': 4.3771404109589045e-05, 'epoch': 1.0}\n",
      "{'eval_loss': 0.5293452739715576, 'eval_precision': 0.8403339255859983, 'eval_recall': 0.8198970840480274, 'eval_f1': 0.8036490574894555, 'eval_accuracy': 0.8198970840480274, 'eval_runtime': 4.6833, 'eval_samples_per_second': 124.485, 'eval_steps_per_second': 4.057, 'epoch': 1.0}\n",
      "{'loss': 0.4213, 'grad_norm': 4.704588890075684, 'learning_rate': 3.752140410958904e-05, 'epoch': 2.0}\n",
      "{'eval_loss': 0.3491709232330322, 'eval_precision': 0.8854746665809452, 'eval_recall': 0.8867924528301887, 'eval_f1': 0.8849866847505855, 'eval_accuracy': 0.8867924528301887, 'eval_runtime': 5.0544, 'eval_samples_per_second': 115.346, 'eval_steps_per_second': 3.759, 'epoch': 2.0}\n",
      "{'loss': 0.3282, 'grad_norm': 2.23502254486084, 'learning_rate': 3.127140410958904e-05, 'epoch': 3.0}\n",
      "{'eval_loss': 0.3043082654476166, 'eval_precision': 0.896749540771018, 'eval_recall': 0.8970840480274442, 'eval_f1': 0.8950472160629852, 'eval_accuracy': 0.8970840480274442, 'eval_runtime': 4.7111, 'eval_samples_per_second': 123.752, 'eval_steps_per_second': 4.033, 'epoch': 3.0}\n",
      "{'loss': 0.2932, 'grad_norm': 3.6331253051757812, 'learning_rate': 2.5021404109589043e-05, 'epoch': 4.0}\n",
      "{'eval_loss': 0.29031094908714294, 'eval_precision': 0.9001265796077117, 'eval_recall': 0.9005145797598628, 'eval_f1': 0.899334516428624, 'eval_accuracy': 0.9005145797598628, 'eval_runtime': 4.828, 'eval_samples_per_second': 120.754, 'eval_steps_per_second': 3.935, 'epoch': 4.0}\n",
      "{'loss': 0.2739, 'grad_norm': 4.636922359466553, 'learning_rate': 1.877140410958904e-05, 'epoch': 5.0}\n",
      "{'eval_loss': 0.28265586495399475, 'eval_precision': 0.9134396414398346, 'eval_recall': 0.9125214408233276, 'eval_f1': 0.9126351871209563, 'eval_accuracy': 0.9125214408233276, 'eval_runtime': 5.0186, 'eval_samples_per_second': 116.169, 'eval_steps_per_second': 3.786, 'epoch': 5.0}\n",
      "{'loss': 0.2537, 'grad_norm': 1.3339991569519043, 'learning_rate': 1.252140410958904e-05, 'epoch': 6.0}\n",
      "{'eval_loss': 0.2768203020095825, 'eval_precision': 0.90875299116922, 'eval_recall': 0.9073756432246999, 'eval_f1': 0.9075634938998797, 'eval_accuracy': 0.9073756432246999, 'eval_runtime': 5.0116, 'eval_samples_per_second': 116.33, 'eval_steps_per_second': 3.791, 'epoch': 6.0}\n",
      "{'loss': 0.2511, 'grad_norm': 6.263134002685547, 'learning_rate': 6.2714041095890405e-06, 'epoch': 7.0}\n",
      "{'eval_loss': 0.2738068997859955, 'eval_precision': 0.9048591479873737, 'eval_recall': 0.9039451114922813, 'eval_f1': 0.9040686520061507, 'eval_accuracy': 0.9039451114922813, 'eval_runtime': 4.6829, 'eval_samples_per_second': 124.496, 'eval_steps_per_second': 4.057, 'epoch': 7.0}\n",
      "{'loss': 0.2476, 'grad_norm': 5.103510856628418, 'learning_rate': 2.1404109589041097e-08, 'epoch': 8.0}\n",
      "{'eval_loss': 0.27267298102378845, 'eval_precision': 0.9063987682719061, 'eval_recall': 0.9056603773584906, 'eval_f1': 0.9055383438912658, 'eval_accuracy': 0.9056603773584906, 'eval_runtime': 4.9469, 'eval_samples_per_second': 117.851, 'eval_steps_per_second': 3.841, 'epoch': 8.0}\n",
      "{'train_runtime': 761.1634, 'train_samples_per_second': 49.02, 'train_steps_per_second': 3.069, 'train_loss': 0.37879799490105615, 'epoch': 8.0}\n",
      "{'eval_loss': 0.29276779294013977, 'eval_precision': 0.8977555629660928, 'eval_recall': 0.8972602739726028, 'eval_f1': 0.8964068416925849, 'eval_accuracy': 0.8972602739726028, 'eval_runtime': 4.8344, 'eval_samples_per_second': 120.802, 'eval_steps_per_second': 3.93, 'epoch': 8.0}\n",
      "\n",
      "=== Prompt Tuning (distilbert-base-uncased) ===\n",
      "Using 20 shots per class (total ~120 samples)\n",
      "trainable params: 15,360 || all params: 66,973,446 || trainable%: 0.0229\n",
      "{'loss': 1.796, 'grad_norm': 0.022897260263562202, 'learning_rate': 4.453125e-05, 'epoch': 1.0}\n",
      "{'eval_loss': 1.735572099685669, 'eval_precision': 0.257348823775589, 'eval_recall': 0.5060034305317325, 'eval_f1': 0.3411777117024893, 'eval_accuracy': 0.5060034305317325, 'eval_runtime': 5.0323, 'eval_samples_per_second': 115.853, 'eval_steps_per_second': 3.776, 'epoch': 1.0}\n",
      "{'loss': 1.7939, 'grad_norm': 0.020090842619538307, 'learning_rate': 3.828125e-05, 'epoch': 2.0}\n",
      "{'eval_loss': 1.735579013824463, 'eval_precision': 0.257348823775589, 'eval_recall': 0.5060034305317325, 'eval_f1': 0.3411777117024893, 'eval_accuracy': 0.5060034305317325, 'eval_runtime': 5.0297, 'eval_samples_per_second': 115.911, 'eval_steps_per_second': 3.778, 'epoch': 2.0}\n",
      "{'loss': 1.8004, 'grad_norm': 0.05354446917772293, 'learning_rate': 3.203125e-05, 'epoch': 3.0}\n",
      "{'eval_loss': 1.7355777025222778, 'eval_precision': 0.257348823775589, 'eval_recall': 0.5060034305317325, 'eval_f1': 0.3411777117024893, 'eval_accuracy': 0.5060034305317325, 'eval_runtime': 5.2096, 'eval_samples_per_second': 111.909, 'eval_steps_per_second': 3.647, 'epoch': 3.0}\n",
      "{'loss': 1.7918, 'grad_norm': 0.021209923550486565, 'learning_rate': 2.578125e-05, 'epoch': 4.0}\n",
      "{'eval_loss': 1.7355835437774658, 'eval_precision': 0.257348823775589, 'eval_recall': 0.5060034305317325, 'eval_f1': 0.3411777117024893, 'eval_accuracy': 0.5060034305317325, 'eval_runtime': 5.1506, 'eval_samples_per_second': 113.19, 'eval_steps_per_second': 3.689, 'epoch': 4.0}\n",
      "{'loss': 1.7975, 'grad_norm': 0.01471839752048254, 'learning_rate': 1.953125e-05, 'epoch': 5.0}\n",
      "{'eval_loss': 1.7355868816375732, 'eval_precision': 0.257348823775589, 'eval_recall': 0.5060034305317325, 'eval_f1': 0.3411777117024893, 'eval_accuracy': 0.5060034305317325, 'eval_runtime': 5.1521, 'eval_samples_per_second': 113.157, 'eval_steps_per_second': 3.688, 'epoch': 5.0}\n",
      "{'loss': 1.8007, 'grad_norm': 0.0209451112896204, 'learning_rate': 1.3281250000000001e-05, 'epoch': 6.0}\n",
      "{'eval_loss': 1.7355881929397583, 'eval_precision': 0.257348823775589, 'eval_recall': 0.5060034305317325, 'eval_f1': 0.3411777117024893, 'eval_accuracy': 0.5060034305317325, 'eval_runtime': 5.2075, 'eval_samples_per_second': 111.955, 'eval_steps_per_second': 3.649, 'epoch': 6.0}\n",
      "{'loss': 1.7917, 'grad_norm': 0.018645912408828735, 'learning_rate': 7.031250000000001e-06, 'epoch': 7.0}\n",
      "{'eval_loss': 1.7355890274047852, 'eval_precision': 0.257348823775589, 'eval_recall': 0.5060034305317325, 'eval_f1': 0.3411777117024893, 'eval_accuracy': 0.5060034305317325, 'eval_runtime': 5.1396, 'eval_samples_per_second': 113.433, 'eval_steps_per_second': 3.697, 'epoch': 7.0}\n",
      "{'loss': 1.7955, 'grad_norm': 0.02963181957602501, 'learning_rate': 7.8125e-07, 'epoch': 8.0}\n",
      "{'eval_loss': 1.7355893850326538, 'eval_precision': 0.257348823775589, 'eval_recall': 0.5060034305317325, 'eval_f1': 0.3411777117024893, 'eval_accuracy': 0.5060034305317325, 'eval_runtime': 5.1819, 'eval_samples_per_second': 112.507, 'eval_steps_per_second': 3.667, 'epoch': 8.0}\n",
      "{'train_runtime': 63.347, 'train_samples_per_second': 15.155, 'train_steps_per_second': 1.01, 'train_loss': 1.7959314286708832, 'epoch': 8.0}\n",
      "{'eval_loss': 1.7354782819747925, 'eval_precision': 0.25689622818540064, 'eval_recall': 0.5068493150684932, 'eval_f1': 0.34097135740971357, 'eval_accuracy': 0.5068493150684932, 'eval_runtime': 5.2382, 'eval_samples_per_second': 111.488, 'eval_steps_per_second': 3.627, 'epoch': 8.0}\n"
     ]
    }
   ],
   "source": [
    "MODELS = [\"bert-base-uncased\", \"roberta-base\", \"distilbert-base-uncased\"]\n",
    "\n",
    "# Run experiments for all models\n",
    "all_results = {}\n",
    "\n",
    "for model_name in MODELS:\n",
    "    # Prepare datasets for current model\n",
    "    train_dataset, val_dataset, test_dataset, tokenizer = prepare_datasets(model_name, max_length=150)\n",
    "    \n",
    "    # Get model key (e.g., 'bert' from 'bert-base-uncased')\n",
    "    model_key = model_name.split('-')[0]\n",
    "    \n",
    "    # Run all methods for current model\n",
    "    model_results = {}\n",
    "    \n",
    "    print(f\"\\n===== Running Experiments for {model_name.upper()} =====\")\n",
    "    model_results['standard_ft'] = run_standard_finetuning(model_name, tokenizer)\n",
    "    model_results['lora_ft'] = run_lora_finetuning(model_name, tokenizer)\n",
    "    model_results['prompt_tuning'] = run_prompt_tuning(model_name, \n",
    "                                                       tokenizer, \n",
    "                                                       train_dataset, \n",
    "                                                       val_dataset, \n",
    "                                                       test_dataset,\n",
    "                                                       shots_per_class=SHOTS_PER_CLASS\n",
    "                                                      )\n",
    "    \n",
    "    all_results[model_key] = model_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32be138c-5368-4a4e-b608-4da4f9fef847",
   "metadata": {},
   "source": [
    "### Performance Comparision of All Models\n",
    "- Compare the performance of all the three models (BERT, RoBERTa and DistilBERT) based on the F1 score.\n",
    "- Display the parameters for all the configurations.\n",
    "- Plot the confusion matrices of each configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2c935870-b3e2-461e-a1af-2f8ab865fef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Performance Comparison of All Models===\n",
      "| Model      | Method        |   F1 Score |   Precision |   Recall | Total Params   | Trainable Params   |\n",
      "|:-----------|:--------------|-----------:|------------:|---------:|:---------------|:-------------------|\n",
      "| Bert       | Standard Ft   |     0.8957 |      0.8996 |   0.8973 | 109,486,854    | 109,486,854        |\n",
      "| Bert       | Lora Ft       |     0.8227 |      0.8295 |   0.839  | 109,786,380    | 299,526            |\n",
      "| Bert       | Prompt Tuning |     0.1032 |      0.4    |   0.2346 | 109,502,214    | 15,360             |\n",
      "| Roberta    | Standard Ft   |     0.8938 |      0.8964 |   0.8955 | 124,650,246    | 124,650,246        |\n",
      "| Roberta    | Lora Ft       |     0.8991 |      0.9032 |   0.9007 | 125,540,364    | 890,118            |\n",
      "| Roberta    | Prompt Tuning |     0.0878 |      0.0557 |   0.2072 | 124,665,606    | 15,360             |\n",
      "| Distilbert | Standard Ft   |     0.9054 |      0.9064 |   0.9058 | 66,958,086     | 66,958,086         |\n",
      "| Distilbert | Lora Ft       |     0.8964 |      0.8978 |   0.8973 | 67,700,748     | 742,662            |\n",
      "| Distilbert | Prompt Tuning |     0.341  |      0.2569 |   0.5068 | 66,973,446     | 15,360             |\n"
     ]
    }
   ],
   "source": [
    "# Comprehensive comparison tables\n",
    "def create_comparison_table(all_results):\n",
    "    table_data = []\n",
    "    \n",
    "    for model_key, methods in all_results.items():\n",
    "        for method, res in methods.items():\n",
    "            row = {\n",
    "                'Model': model_key.capitalize(),\n",
    "                'Method': method.replace('_', ' ').title(),\n",
    "                'F1 Score': f\"{res['metrics']['eval_f1']:.4f}\",\n",
    "                'Precision': f\"{res['metrics']['eval_precision']:.4f}\",\n",
    "                'Recall': f\"{res['metrics']['eval_recall']:.4f}\",\n",
    "                'Total Params': f\"{res['num_params']:,}\",\n",
    "                'Trainable Params': f\"{res.get('trainable_params', res['num_params']):,}\"\n",
    "            }\n",
    "            table_data.append(row)\n",
    "    \n",
    "    return pd.DataFrame(table_data)\n",
    "\n",
    "\n",
    "comparison_table = create_comparison_table(all_results)\n",
    "\n",
    "\n",
    "print(\"\\n=== Performance Comparison of All Models===\")\n",
    "print(comparison_table.to_markdown(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0871fa29-8a1c-4fdc-a3b3-7ba30536b352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrices saved in 'confusion_matrices' directory\n",
      "Confusion matrices saved in 'confusion_matrices' directory\n",
      "Confusion matrices saved in 'confusion_matrices' directory\n",
      "Confusion matrices saved in 'confusion_matrices' directory\n",
      "Confusion matrices saved in 'confusion_matrices' directory\n",
      "Confusion matrices saved in 'confusion_matrices' directory\n",
      "Confusion matrices saved in 'confusion_matrices' directory\n",
      "Confusion matrices saved in 'confusion_matrices' directory\n",
      "Confusion matrices saved in 'confusion_matrices' directory\n"
     ]
    }
   ],
   "source": [
    "# Class mapping with proper names\n",
    "class_mapping = {\n",
    "    0: 'non_damage',\n",
    "    1: 'damaged_infrastructure', \n",
    "    2: 'damaged_nature',\n",
    "    3: 'fires',\n",
    "    4: 'flood',\n",
    "    5: 'human_damage'\n",
    "}\n",
    "\n",
    "# Get the class names in correct order\n",
    "class_names = [class_mapping[i] for i in sorted(class_mapping.keys())]\n",
    "\n",
    "# Plot all the confusion matrices\n",
    "results = []\n",
    "for model_key, methods in all_results.items():\n",
    "    for method, res in methods.items():\n",
    "        trainer = Trainer(\n",
    "            model=res['model'],\n",
    "            args=TrainingArguments(\n",
    "                output_dir='./temp',\n",
    "                per_device_eval_batch_size=32,\n",
    "                report_to=\"none\",\n",
    "                logging_strategy=\"epoch\",\n",
    "                dataloader_pin_memory=False\n",
    "            ),\n",
    "            compute_metrics=compute_metrics\n",
    "        )\n",
    "        preds = trainer.predict(test_dataset)\n",
    "        results.append({\n",
    "            'model': model_key,\n",
    "            'method': method,\n",
    "            'y_true': preds.label_ids,\n",
    "            'y_pred': np.argmax(preds.predictions, axis=1)\n",
    "        })\n",
    "\n",
    "\n",
    "for result in results:\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    cm = confusion_matrix(result['y_true'], result['y_pred'])\n",
    "    \n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=class_names,\n",
    "                yticklabels=class_names, \n",
    "                cbar=False)\n",
    "    \n",
    "    # Customize plot\n",
    "    plt.xlabel('Predicted Disaster Type', fontsize=12)\n",
    "    plt.ylabel('Actual Disaster Type', fontsize=12)\n",
    "    plt.title(f\"{result['model'].upper()} - {method.replace('_', ' ').title()}\", \n",
    "              fontsize=14, pad=20)\n",
    "    \n",
    "    # Rotate labels for better readability\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    os.makedirs('confusion_matrices', exist_ok=True)\n",
    "    filename = f\"{result['model']}_{result['method']}_cm.png\"\n",
    "    plt.savefig(f'confusion_matrices/{filename}', \n",
    "               bbox_inches='tight', dpi=700)\n",
    "    print(\"Confusion matrices saved in 'confusion_matrices' directory\")\n",
    "    # plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "367b6ed8-b8b2-465a-846a-b3a09ee69b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot confusion matrix\n",
    "def plot_confusion_matrix(y_true, y_pred, class_names, title):\n",
    "    \"\"\"\n",
    "    Plot confusion matrix using seaborn heatmap\n",
    "    \n",
    "    Args:\n",
    "        y_true: Array of true labels\n",
    "        y_pred: Array of predicted labels\n",
    "        class_names: List of class names\n",
    "        title: Title for the plot\n",
    "    \"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=class_names, \n",
    "                yticklabels=class_names)\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    plt.title(title)\n",
    "    filename = \"majority_voting_ensemble_cm.png\"\n",
    "    plt.savefig(f'confusion_matrices/{filename}', bbox_inches='tight', dpi=700)\n",
    "    print(f\"Saved the confusion matrix with title - {title}\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266e9b5e-73db-4c34-92cf-b4a7c82014d5",
   "metadata": {},
   "source": [
    "### Perform Majority Voting\n",
    "- Take the predictions of the three best (based on weighted F1 score) models of each configurations (standard fine-tuning, LoRa and few-shot learning) for each sample of the test set and predict the class based on the predictions of the majority voting of the models.\n",
    "- In case tie, choose the predictions of the best model (not applicable in this scenerio as we have odd number of models).\n",
    "- Display the output of all configurations in a table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0f1bfc53-3af6-4410-8989-9b194c210659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best models selected for majority voting based on F1 score:\n",
      "METHOD: standard \tF1 score: 0.9054\n",
      "METHOD: lora \tF1 score: 0.8991\n",
      "METHOD: prompt \tF1 score: 0.3410\n",
      "Saved the confusion matrix with title - Majority Voting Ensemble\n",
      "\n",
      "=== Updated Performance Comparison ===\n",
      "| Model      | Method          |   F1 Score |   Precision |   Recall | Total Params   | Trainable Params   |\n",
      "|:-----------|:----------------|-----------:|------------:|---------:|:---------------|:-------------------|\n",
      "| Bert       | Standard Ft     |     0.8957 |      0.8996 |   0.8973 | 109,486,854    | 109,486,854        |\n",
      "| Bert       | Lora Ft         |     0.8227 |      0.8295 |   0.839  | 109,786,380    | 299,526            |\n",
      "| Bert       | Prompt Tuning   |     0.1032 |      0.4    |   0.2346 | 109,502,214    | 15,360             |\n",
      "| Roberta    | Standard Ft     |     0.8938 |      0.8964 |   0.8955 | 124,650,246    | 124,650,246        |\n",
      "| Roberta    | Lora Ft         |     0.8991 |      0.9032 |   0.9007 | 125,540,364    | 890,118            |\n",
      "| Roberta    | Prompt Tuning   |     0.0878 |      0.0557 |   0.2072 | 124,665,606    | 15,360             |\n",
      "| Distilbert | Standard Ft     |     0.9054 |      0.9064 |   0.9058 | 66,958,086     | 66,958,086         |\n",
      "| Distilbert | Lora Ft         |     0.8964 |      0.8978 |   0.8973 | 67,700,748     | 742,662            |\n",
      "| Distilbert | Prompt Tuning   |     0.341  |      0.2569 |   0.5068 | 66,973,446     | 15,360             |\n",
      "| Ensemble   | Majority Voting |     0.5541 |      0.7069 |   0.6216 | N/A            | N/A                |\n",
      "\n",
      "=== All experiments completed ===\n",
      "Results saved in 'results' directory\n"
     ]
    }
   ],
   "source": [
    "# Majority voting using the best models of all three fine-tuning approaches\n",
    "def majority_voting(all_results, test_dataset):\n",
    "    # Get the best model of each type (standard, lora, prompt) across all architectures\n",
    "    best_models = {\n",
    "        'standard': None,\n",
    "        'lora': None,\n",
    "        'prompt': None\n",
    "    }\n",
    "    \n",
    "    # Find best performing model for each method type\n",
    "    best_f1 = {'standard': -1, 'lora': -1, 'prompt': -1}\n",
    "    \n",
    "    for model_key, methods in all_results.items():\n",
    "        # Standard fine-tuning\n",
    "        if methods['standard_ft']['metrics']['eval_f1'] > best_f1['standard']:\n",
    "            best_f1['standard'] = methods['standard_ft']['metrics']['eval_f1']\n",
    "            best_models['standard'] = methods['standard_ft']['model']\n",
    "        \n",
    "        # LoRA fine-tuning\n",
    "        if methods['lora_ft']['metrics']['eval_f1'] > best_f1['lora']:\n",
    "            best_f1['lora'] = methods['lora_ft']['metrics']['eval_f1']\n",
    "            best_models['lora'] = methods['lora_ft']['model']\n",
    "        \n",
    "        # Prompt tuning\n",
    "        if methods['prompt_tuning']['metrics']['eval_f1'] > best_f1['prompt']:\n",
    "            best_f1['prompt'] = methods['prompt_tuning']['metrics']['eval_f1']\n",
    "            best_models['prompt'] = methods['prompt_tuning']['model']\n",
    "    \n",
    "    print(\"\\nBest models selected for majority voting based on F1 score:\")\n",
    "    for method, model in best_models.items():\n",
    "        print(f\"METHOD: {method} \\tF1 score: {best_f1[method]:.4f}\")\n",
    "    \n",
    "    # Get predictions from each best model\n",
    "    all_preds = []\n",
    "    for method, model in best_models.items():\n",
    "        trainer = Trainer(model=model, args=TrainingArguments(dataloader_pin_memory=False))\n",
    "        preds = trainer.predict(test_dataset)\n",
    "        class_preds = np.argmax(preds.predictions, axis=1)\n",
    "        all_preds.append(class_preds)\n",
    "    \n",
    "    # Perform majority voting\n",
    "    final_preds = []\n",
    "    for i in range(len(test_dataset)):\n",
    "        votes = [pred[i] for pred in all_preds]\n",
    "        \n",
    "        # Count votes and select the most common\n",
    "        vote_counts = Counter(votes)\n",
    "        most_common = vote_counts.most_common(1)[0]\n",
    "        \n",
    "        # If tie, select the prediction from the best performing model (standard FT)\n",
    "        if len(vote_counts) > 1 and vote_counts.most_common(2)[0][1] == vote_counts.most_common(2)[1][1]:\n",
    "            final_preds.append(all_preds[0][i])  # standard FT breaks ties\n",
    "        else:\n",
    "            final_preds.append(most_common[0])\n",
    "    \n",
    "    return np.array(final_preds)\n",
    "\n",
    "# Get true labels from test dataset\n",
    "def get_true_labels(test_dataset):\n",
    "    return np.array([item['labels'].item() for item in test_dataset])\n",
    "\n",
    "# Run majority voting\n",
    "y_pred_majority = majority_voting(all_results, test_dataset)\n",
    "y_true = get_true_labels(test_dataset)\n",
    "\n",
    "# Plot the confusion matrix for ensemble method\n",
    "plot_confusion_matrix(y_true, y_pred_majority, class_names, \"Majority Voting Ensemble\")\n",
    "\n",
    "# Add majority voting results to comparison table\n",
    "mv_metrics = precision_recall_fscore_support(y_true, y_pred_majority, average='weighted')\n",
    "\n",
    "# Add the result of the ensemble approach at the end of the table\n",
    "comparison_table.loc[len(comparison_table)] = {\n",
    "    'Model': 'Ensemble',\n",
    "    'Method': 'Majority Voting',\n",
    "    'F1 Score': f\"{mv_metrics[2]:.4f}\",\n",
    "    'Precision': f\"{mv_metrics[0]:.4f}\",\n",
    "    'Recall': f\"{mv_metrics[1]:.4f}\",\n",
    "    'Total Params': 'N/A',\n",
    "    'Trainable Params': 'N/A'\n",
    "}\n",
    "\n",
    "print(\"\\n=== Updated Performance Comparison ===\")\n",
    "print(comparison_table.to_markdown(index=False))\n",
    "\n",
    "# Save the table in a csv file\n",
    "os.makedirs('results', exist_ok=True)\n",
    "comparison_table.to_csv('results/full_comparison.csv', index=False)\n",
    "\n",
    "print(\"\\n=== All experiments completed ===\")\n",
    "print(\"Results saved in 'results' directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4240efe6-23d2-425b-949b-2bd0f20853b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best models selected for majority voting (Standard + LoRA only):\n",
      "Standard FT: F1 = 0.9054\n",
      "LoRA FT: F1 = 0.8991\n",
      "\n",
      "=== Majority Voting (Standard + LoRA) Performance ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.73      0.84       296\n",
      "           1       0.38      0.98      0.55       139\n",
      "           2       1.00      0.04      0.07        52\n",
      "           3       0.50      0.03      0.06        34\n",
      "           4       0.00      0.00      0.00        39\n",
      "           5       1.00      0.08      0.15        24\n",
      "\n",
      "    accuracy                           0.61       584\n",
      "   macro avg       0.64      0.31      0.28       584\n",
      "weighted avg       0.75      0.61      0.57       584\n",
      "\n",
      "Saved the confusion matrix with title - Majority Voting (Standard+LoRA)\n",
      "\n",
      "=== Final Performance Comparison ===\n",
      "| Model      | Method                     |   F1 Score |   Precision |   Recall | Total Params   | Trainable Params   |\n",
      "|:-----------|:---------------------------|-----------:|------------:|---------:|:---------------|:-------------------|\n",
      "| Bert       | Standard Ft                |     0.8957 |      0.8996 |   0.8973 | 109,486,854    | 109,486,854        |\n",
      "| Bert       | Lora Ft                    |     0.8227 |      0.8295 |   0.839  | 109,786,380    | 299,526            |\n",
      "| Bert       | Prompt Tuning              |     0.1032 |      0.4    |   0.2346 | 109,502,214    | 15,360             |\n",
      "| Roberta    | Standard Ft                |     0.8938 |      0.8964 |   0.8955 | 124,650,246    | 124,650,246        |\n",
      "| Roberta    | Lora Ft                    |     0.8991 |      0.9032 |   0.9007 | 125,540,364    | 890,118            |\n",
      "| Roberta    | Prompt Tuning              |     0.0878 |      0.0557 |   0.2072 | 124,665,606    | 15,360             |\n",
      "| Distilbert | Standard Ft                |     0.9054 |      0.9064 |   0.9058 | 66,958,086     | 66,958,086         |\n",
      "| Distilbert | Lora Ft                    |     0.8964 |      0.8978 |   0.8973 | 67,700,748     | 742,662            |\n",
      "| Distilbert | Prompt Tuning              |     0.341  |      0.2569 |   0.5068 | 66,973,446     | 15,360             |\n",
      "| Ensemble   | Majority Voting            |     0.5541 |      0.7069 |   0.6216 | N/A            | N/A                |\n",
      "| Ensemble   | Majority Voting (Std+LoRA) |     0.5708 |      0.7473 |   0.6113 | N/A            | N/A                |\n"
     ]
    }
   ],
   "source": [
    "# Majority voting based on only standard and LoRA fine-tuning\n",
    "def majority_voting_standard_lora(all_results, test_dataset):\n",
    "    # Get the best model of each type (standard and lora only)\n",
    "    best_models = {\n",
    "        'standard': None,\n",
    "        'lora': None\n",
    "    }\n",
    "    \n",
    "    # Find best performing model for each method type\n",
    "    best_f1 = {'standard': -1, 'lora': -1}\n",
    "    \n",
    "    for model_key, methods in all_results.items():\n",
    "        # Standard fine-tuning\n",
    "        if methods['standard_ft']['metrics']['eval_f1'] > best_f1['standard']:\n",
    "            best_f1['standard'] = methods['standard_ft']['metrics']['eval_f1']\n",
    "            best_models['standard'] = methods['standard_ft']['model']\n",
    "        \n",
    "        # LoRA fine-tuning only (skip prompt tuning)\n",
    "        if methods['lora_ft']['metrics']['eval_f1'] > best_f1['lora']:\n",
    "            best_f1['lora'] = methods['lora_ft']['metrics']['eval_f1']\n",
    "            best_models['lora'] = methods['lora_ft']['model']\n",
    "    \n",
    "    print(\"\\nBest models selected for majority voting (Standard + LoRA only):\")\n",
    "    print(f\"Standard FT: F1 = {best_f1['standard']:.4f}\")\n",
    "    print(f\"LoRA FT: F1 = {best_f1['lora']:.4f}\")\n",
    "    \n",
    "    # Get predictions from each best model\n",
    "    all_preds = []\n",
    "    for method, model in best_models.items():\n",
    "        trainer = Trainer(model=model, args=TrainingArguments(dataloader_pin_memory=False))\n",
    "        preds = trainer.predict(test_dataset)\n",
    "        class_preds = np.argmax(preds.predictions, axis=1)\n",
    "        all_preds.append(class_preds)\n",
    "    \n",
    "    # Perform majority voting between two models\n",
    "    final_preds = []\n",
    "    for pred1, pred2 in zip(all_preds[0], all_preds[1]):\n",
    "        if pred1 == pred2:  # Agreement\n",
    "            final_preds.append(pred1)\n",
    "        else:  # Disagreement - use the better performing model (standard FT)\n",
    "            final_preds.append(all_preds[0][i])  # standard FT breaks ties\n",
    "    \n",
    "    return np.array(final_preds)\n",
    "\n",
    "# Run majority voting with only standard and LoRA\n",
    "y_pred_majority_std_lora = majority_voting_standard_lora(all_results, test_dataset)\n",
    "\n",
    "# Evaluate performance\n",
    "print(\"\\n=== Majority Voting (Standard + LoRA) Performance ===\")\n",
    "print(classification_report(y_true, y_pred_majority_std_lora))\n",
    "\n",
    "# Plot confusion matrix\n",
    "plot_confusion_matrix(y_true, y_pred_majority_std_lora, class_names, \"Majority Voting (Standard+LoRA)\")\n",
    "\n",
    "# Add to comparison table\n",
    "mv_metrics = precision_recall_fscore_support(y_true, y_pred_majority_std_lora, average='weighted')\n",
    "comparison_table.loc[len(comparison_table)] = {\n",
    "    'Model': 'Ensemble',\n",
    "    'Method': 'Majority Voting (Std+LoRA)',\n",
    "    'F1 Score': f\"{mv_metrics[2]:.4f}\",\n",
    "    'Precision': f\"{mv_metrics[0]:.4f}\",\n",
    "    'Recall': f\"{mv_metrics[1]:.4f}\",\n",
    "    'Total Params': 'N/A',\n",
    "    'Trainable Params': 'N/A'\n",
    "}\n",
    "\n",
    "# Save updated results\n",
    "comparison_table.to_csv('results/full_comparison.csv', index=False)\n",
    "print(\"\\n=== Final Performance Comparison ===\")\n",
    "print(comparison_table.to_markdown(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "15d4d9ac-b383-4b5b-9c0a-7e09f90e0975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best models selected for weighted voting:\n",
      "Standard FT: F1 = 0.9054 (Weight: 0.91)\n",
      "LoRA FT: F1 = 0.8991 (Weight: 0.90)\n",
      "\n",
      "=== Weighted Voting Performance ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.99      0.91       296\n",
      "           1       0.85      0.79      0.82       139\n",
      "           2       0.87      0.65      0.75        52\n",
      "           3       0.92      0.65      0.76        34\n",
      "           4       0.92      0.56      0.70        39\n",
      "           5       0.75      0.62      0.68        24\n",
      "\n",
      "    accuracy                           0.85       584\n",
      "   macro avg       0.86      0.71      0.77       584\n",
      "weighted avg       0.85      0.85      0.84       584\n",
      "\n",
      "Saved the confusion matrix with title - Weighted Voting (Standard+LoRA)\n",
      "\n",
      "=== Final Performance Comparison ===\n",
      "| Model      | Method                     |   F1 Score |   Precision |   Recall | Total Params   | Trainable Params   |\n",
      "|:-----------|:---------------------------|-----------:|------------:|---------:|:---------------|:-------------------|\n",
      "| Bert       | Standard Ft                |     0.8957 |      0.8996 |   0.8973 | 109,486,854    | 109,486,854        |\n",
      "| Bert       | Lora Ft                    |     0.8227 |      0.8295 |   0.839  | 109,786,380    | 299,526            |\n",
      "| Bert       | Prompt Tuning              |     0.1032 |      0.4    |   0.2346 | 109,502,214    | 15,360             |\n",
      "| Roberta    | Standard Ft                |     0.8938 |      0.8964 |   0.8955 | 124,650,246    | 124,650,246        |\n",
      "| Roberta    | Lora Ft                    |     0.8991 |      0.9032 |   0.9007 | 125,540,364    | 890,118            |\n",
      "| Roberta    | Prompt Tuning              |     0.0878 |      0.0557 |   0.2072 | 124,665,606    | 15,360             |\n",
      "| Distilbert | Standard Ft                |     0.9054 |      0.9064 |   0.9058 | 66,958,086     | 66,958,086         |\n",
      "| Distilbert | Lora Ft                    |     0.8964 |      0.8978 |   0.8973 | 67,700,748     | 742,662            |\n",
      "| Distilbert | Prompt Tuning              |     0.341  |      0.2569 |   0.5068 | 66,973,446     | 15,360             |\n",
      "| Ensemble   | Majority Voting            |     0.5541 |      0.7069 |   0.6216 | N/A            | N/A                |\n",
      "| Ensemble   | Majority Voting (Std+LoRA) |     0.5708 |      0.7473 |   0.6113 | N/A            | N/A                |\n",
      "| Ensemble   | Weighted Voting            |     0.8435 |      0.8539 |   0.851  | N/A            | N/A                |\n"
     ]
    }
   ],
   "source": [
    "# Majority voting based on weighted F1 score\n",
    "def weighted_voting(all_results, test_dataset):\n",
    "    # Get the best model of each type (standard and lora only)\n",
    "    best_models = {\n",
    "        'standard': None,\n",
    "        'lora': None\n",
    "    }\n",
    "    \n",
    "    # Find best performing model for each method type with their F1 scores\n",
    "    best_f1 = {'standard': -1, 'lora': -1}\n",
    "    \n",
    "    for model_key, methods in all_results.items():\n",
    "        # Standard fine-tuning\n",
    "        if methods['standard_ft']['metrics']['eval_f1'] > best_f1['standard']:\n",
    "            best_f1['standard'] = methods['standard_ft']['metrics']['eval_f1']\n",
    "            best_models['standard'] = methods['standard_ft']['model']\n",
    "        \n",
    "        # LoRA fine-tuning only\n",
    "        if methods['lora_ft']['metrics']['eval_f1'] > best_f1['lora']:\n",
    "            best_f1['lora'] = methods['lora_ft']['metrics']['eval_f1']\n",
    "            best_models['lora'] = methods['lora_ft']['model']\n",
    "    \n",
    "    print(\"\\nBest models selected for weighted voting:\")\n",
    "    print(f\"Standard FT: F1 = {best_f1['standard']:.4f} (Weight: {best_f1['standard']:.2f})\")\n",
    "    print(f\"LoRA FT: F1 = {best_f1['lora']:.4f} (Weight: {best_f1['lora']:.2f})\")\n",
    "    \n",
    "    # Get predictions and probabilities from each best model\n",
    "    all_preds = []\n",
    "    all_probs = []\n",
    "    for method, model in best_models.items():\n",
    "        trainer = Trainer(model=model, args=TrainingArguments(dataloader_pin_memory=False))\n",
    "        preds = trainer.predict(test_dataset)\n",
    "        class_preds = np.argmax(preds.predictions, axis=1)\n",
    "        class_probs = torch.softmax(torch.Tensor(preds.predictions), dim=1).numpy()\n",
    "        all_preds.append(class_preds)\n",
    "        all_probs.append(class_probs)\n",
    "    \n",
    "    # Calculate weights (normalized F1 scores)\n",
    "    total_weight = best_f1['standard'] + best_f1['lora']\n",
    "    weights = {\n",
    "        'standard': best_f1['standard'] / total_weight,\n",
    "        'lora': best_f1['lora'] / total_weight\n",
    "    }\n",
    "    \n",
    "    # Perform weighted voting\n",
    "    final_preds = []\n",
    "    for i in range(len(test_dataset)):\n",
    "        # Weighted probability aggregation\n",
    "        weighted_probs = np.zeros_like(all_probs[0][i])\n",
    "        for j, (probs, method) in enumerate(zip(all_probs, ['standard', 'lora'])):\n",
    "            weighted_probs += probs[i] * weights[method]\n",
    "        \n",
    "        final_preds.append(np.argmax(weighted_probs))\n",
    "    \n",
    "    return np.array(final_preds)\n",
    "\n",
    "# Run weighted voting\n",
    "y_pred_weighted = weighted_voting(all_results, test_dataset)\n",
    "\n",
    "# Evaluate performance\n",
    "print(\"\\n=== Weighted Voting Performance ===\")\n",
    "print(classification_report(y_true, y_pred_weighted))\n",
    "\n",
    "# Plot confusion matrix\n",
    "plot_confusion_matrix(y_true, y_pred_weighted, class_names, \"Weighted Voting (Standard+LoRA)\")\n",
    "\n",
    "# Add to comparison table\n",
    "wv_metrics = precision_recall_fscore_support(y_true, y_pred_weighted, average='weighted')\n",
    "comparison_table.loc[len(comparison_table)] = {\n",
    "    'Model': 'Ensemble',\n",
    "    'Method': 'Weighted Voting',\n",
    "    'F1 Score': f\"{wv_metrics[2]:.4f}\",\n",
    "    'Precision': f\"{wv_metrics[0]:.4f}\",\n",
    "    'Recall': f\"{wv_metrics[1]:.4f}\",\n",
    "    'Total Params': 'N/A',\n",
    "    'Trainable Params': 'N/A'\n",
    "}\n",
    "\n",
    "# Save updated results\n",
    "comparison_table.to_csv('results/full_comparison.csv', index=False)\n",
    "print(\"\\n=== Final Performance Comparison ===\")\n",
    "print(comparison_table.to_markdown(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d8388941-05c1-48e6-aa15-8672d7928968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best models selected for weighted voting:\n",
      "Standard FT: F1 = 0.9054 (Weight: 0.91)\n",
      "LoRA FT: F1 = 0.8991 (Weight: 0.90)\n"
     ]
    }
   ],
   "source": [
    "def get_the_best_model(all_results, test_dataset):\n",
    "    best_models = {\n",
    "        'standard': None,\n",
    "        'lora': None\n",
    "    }\n",
    "    \n",
    "    # Find best performing model for each method type with their F1 scores\n",
    "    best_f1 = {'standard': -1, 'lora': -1}\n",
    "    \n",
    "    for model_key, methods in all_results.items():\n",
    "        # Standard fine-tuning\n",
    "        if methods['standard_ft']['metrics']['eval_f1'] > best_f1['standard']:\n",
    "            best_f1['standard'] = methods['standard_ft']['metrics']['eval_f1']\n",
    "            best_models['standard'] = methods['standard_ft']['model']\n",
    "        \n",
    "        # LoRA fine-tuning only\n",
    "        if methods['lora_ft']['metrics']['eval_f1'] > best_f1['lora']:\n",
    "            best_f1['lora'] = methods['lora_ft']['metrics']['eval_f1']\n",
    "            best_models['lora'] = methods['lora_ft']['model']\n",
    "    \n",
    "    print(\"\\nBest models selected for weighted voting:\")\n",
    "    print(f\"Standard FT: F1 = {best_f1['standard']:.4f} (Weight: {best_f1['standard']:.2f})\")\n",
    "    print(f\"LoRA FT: F1 = {best_f1['lora']:.4f} (Weight: {best_f1['lora']:.2f})\")\n",
    "\n",
    "    return best_models\n",
    "\n",
    "    \n",
    "best_models = get_the_best_model(all_results, test_dataset)\n",
    "\n",
    "best_standard_model = best_models.get('standard')\n",
    "best_lora_model = best_models.get('lora')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "70487c44-05a7-41f0-aa95-b821a5c3f1a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Logistic Regression Meta-Learner Performance ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       296\n",
      "           1       0.84      0.86      0.85       139\n",
      "           2       0.84      0.79      0.81        52\n",
      "           3       0.86      0.74      0.79        34\n",
      "           4       0.86      0.77      0.81        39\n",
      "           5       0.69      0.83      0.75        24\n",
      "\n",
      "    accuracy                           0.90       584\n",
      "   macro avg       0.84      0.83      0.83       584\n",
      "weighted avg       0.90      0.90      0.90       584\n",
      "\n",
      "\n",
      "=== SVM Meta-Learner Performance ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99       296\n",
      "           1       0.82      0.86      0.84       139\n",
      "           2       0.87      0.77      0.82        52\n",
      "           3       0.87      0.79      0.83        34\n",
      "           4       0.86      0.77      0.81        39\n",
      "           5       0.69      0.83      0.75        24\n",
      "\n",
      "    accuracy                           0.91       584\n",
      "   macro avg       0.85      0.84      0.84       584\n",
      "weighted avg       0.91      0.91      0.91       584\n",
      "\n",
      "\n",
      "=== Random Forest Meta-Learner Performance ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.98       296\n",
      "           1       0.80      0.88      0.84       139\n",
      "           2       0.90      0.71      0.80        52\n",
      "           3       0.84      0.79      0.82        34\n",
      "           4       0.83      0.77      0.80        39\n",
      "           5       0.70      0.79      0.75        24\n",
      "\n",
      "    accuracy                           0.90       584\n",
      "   macro avg       0.85      0.82      0.83       584\n",
      "weighted avg       0.91      0.90      0.90       584\n",
      "\n",
      "\n",
      "=== Final Performance Comparison ===\n",
      "| Model      | Method                        |   F1 Score |   Precision |   Recall | Total Params   | Trainable Params   |\n",
      "|:-----------|:------------------------------|-----------:|------------:|---------:|:---------------|:-------------------|\n",
      "| Bert       | Standard Ft                   |     0.8957 |      0.8996 |   0.8973 | 109,486,854    | 109,486,854        |\n",
      "| Bert       | Lora Ft                       |     0.8227 |      0.8295 |   0.839  | 109,786,380    | 299,526            |\n",
      "| Bert       | Prompt Tuning                 |     0.1032 |      0.4    |   0.2346 | 109,502,214    | 15,360             |\n",
      "| Roberta    | Standard Ft                   |     0.8938 |      0.8964 |   0.8955 | 124,650,246    | 124,650,246        |\n",
      "| Roberta    | Lora Ft                       |     0.8991 |      0.9032 |   0.9007 | 125,540,364    | 890,118            |\n",
      "| Roberta    | Prompt Tuning                 |     0.0878 |      0.0557 |   0.2072 | 124,665,606    | 15,360             |\n",
      "| Distilbert | Standard Ft                   |     0.9054 |      0.9064 |   0.9058 | 66,958,086     | 66,958,086         |\n",
      "| Distilbert | Lora Ft                       |     0.8964 |      0.8978 |   0.8973 | 67,700,748     | 742,662            |\n",
      "| Distilbert | Prompt Tuning                 |     0.341  |      0.2569 |   0.5068 | 66,973,446     | 15,360             |\n",
      "| Ensemble   | Majority Voting               |     0.5541 |      0.7069 |   0.6216 | N/A            | N/A                |\n",
      "| Ensemble   | Majority Voting (Std+LoRA)    |     0.5708 |      0.7473 |   0.6113 | N/A            | N/A                |\n",
      "| Ensemble   | Weighted Voting               |     0.8435 |      0.8539 |   0.851  | N/A            | N/A                |\n",
      "| Ensemble   | Stacked (Logistic Regression) |     0.9035 |      0.9045 |   0.9041 | N/A            | N/A                |\n",
      "| Ensemble   | Stacked (SVM)                 |     0.9075 |      0.9094 |   0.9075 | N/A            | N/A                |\n",
      "| Ensemble   | Stacked (Random Forest)       |     0.9022 |      0.9054 |   0.9024 | N/A            | N/A                |\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def get_model_predictions(model, dataset):\n",
    "    \"\"\"Get prediction probabilities from a HF model\"\"\"\n",
    "    trainer = Trainer(model=model)\n",
    "    preds = trainer.predict(dataset)\n",
    "    return torch.softmax(torch.Tensor(preds.predictions), dim=1).numpy()\n",
    "\n",
    "\n",
    "# Get probabilities from both models\n",
    "standard_probs = get_model_predictions(best_standard_model, train_dataset)\n",
    "lora_probs = get_model_predictions(best_lora_model, train_dataset)\n",
    "\n",
    "# Stack horizontally to create meta-features\n",
    "meta_features = np.hstack([standard_probs, lora_probs])  # Shape: [n_samples, n_classes*2]\n",
    "\n",
    "# Get true labels\n",
    "y_true_train = np.array([item['labels'].item() for item in train_dataset])\n",
    "y_true_test = np.array([item['labels'].item() for item in test_dataset])\n",
    "\n",
    "# Initialize meta-learners\n",
    "meta_learners = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "    \"SVM\": SVC(kernel='linear', probability=True, random_state=42),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "# Scale features \n",
    "scaler = StandardScaler()\n",
    "meta_features_scaled = scaler.fit_transform(meta_features)\n",
    "\n",
    "# Dictionary to store all predictions\n",
    "stacked_predictions = {}\n",
    "\n",
    "# Train and evaluate each meta-learner\n",
    "for name, model in meta_learners.items():\n",
    "    # Scale features for SVM only\n",
    "    if name == \"SVM\":\n",
    "        X_train = meta_features_scaled\n",
    "    else:\n",
    "        X_train = meta_features\n",
    "        \n",
    "    model.fit(X_train, y_true_train)\n",
    "    \n",
    "    # Prediction function for each meta-learner\n",
    "    def predict_fn(test_dataset, model=model, scaler=scaler if name == \"SVM\" else None):\n",
    "        standard_test_probs = get_model_predictions(best_standard_model, test_dataset)\n",
    "        lora_test_probs = get_model_predictions(best_lora_model, test_dataset)\n",
    "        test_meta_features = np.hstack([standard_test_probs, lora_test_probs])\n",
    "        \n",
    "        if scaler:\n",
    "            test_meta_features = scaler.transform(test_meta_features)\n",
    "            \n",
    "        return model.predict(test_meta_features)\n",
    "    \n",
    "    # Store predictions\n",
    "    stacked_predictions[name] = predict_fn(test_dataset)\n",
    "    \n",
    "    # Evaluate\n",
    "    print(f\"\\n=== {name} Meta-Learner Performance ===\")\n",
    "    print(classification_report(y_true_test, stacked_predictions[name]))\n",
    "\n",
    "# Add results to comparison table\n",
    "for name, preds in stacked_predictions.items():\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        y_true_test, preds, average='weighted'\n",
    "    )\n",
    "    \n",
    "    comparison_table.loc[len(comparison_table)] = {\n",
    "        'Model': 'Ensemble',\n",
    "        'Method': f'Stacked ({name})',\n",
    "        'F1 Score': f\"{f1:.4f}\",\n",
    "        'Precision': f\"{precision:.4f}\",\n",
    "        'Recall': f\"{recall:.4f}\",\n",
    "        'Total Params': 'N/A',\n",
    "        'Trainable Params': 'N/A'\n",
    "    }\n",
    "\n",
    "# Save updated results\n",
    "comparison_table.to_csv('results/full_comparison.csv', index=False)\n",
    "\n",
    "# Display final table\n",
    "print(\"\\n=== Final Performance Comparison ===\")\n",
    "print(comparison_table.to_markdown(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf267b6b-38db-46f0-b310-7c33b14c3daa",
   "metadata": {},
   "source": [
    "### Plot the Misclassification Report\n",
    "- Plots the rate of missclassification (0-1) scale of each class for the best models in each configuration (standard fine-tuning, LoRa, few-shot learning and majority voting) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "242d0ffe-4bf2-4614-ac18-35357940bf3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Misclassification rate plot\n",
    "def get_model_predictions(all_results, test_dataset):\n",
    "    # Get the best model of each type\n",
    "    best_models = {\n",
    "        'Standard FT': None,\n",
    "        'LoRA FT': None,\n",
    "        'Prompt Tuning': None\n",
    "    }\n",
    "    \n",
    "    best_f1 = {'Standard FT': -1, 'LoRA FT': -1, 'Prompt Tuning': -1}\n",
    "    \n",
    "    for model_key, methods in all_results.items():\n",
    "        # Standard fine-tuning\n",
    "        if methods['standard_ft']['metrics']['eval_f1'] > best_f1['Standard FT']:\n",
    "            best_f1['Standard FT'] = methods['standard_ft']['metrics']['eval_f1']\n",
    "            best_models['Standard FT'] = methods['standard_ft']['model']\n",
    "        \n",
    "        # LoRA fine-tuning\n",
    "        if methods['lora_ft']['metrics']['eval_f1'] > best_f1['LoRA FT']:\n",
    "            best_f1['LoRA FT'] = methods['lora_ft']['metrics']['eval_f1']\n",
    "            best_models['LoRA FT'] = methods['lora_ft']['model']\n",
    "        \n",
    "        # Prompt tuning\n",
    "        if methods['prompt_tuning']['metrics']['eval_f1'] > best_f1['Prompt Tuning']:\n",
    "            best_f1['Prompt Tuning'] = methods['prompt_tuning']['metrics']['eval_f1']\n",
    "            best_models['Prompt Tuning'] = methods['prompt_tuning']['model']\n",
    "    \n",
    "    # Get predictions from each model\n",
    "    predictions = {}\n",
    "    for model_name, model in best_models.items():\n",
    "        trainer = Trainer(model=model, args=TrainingArguments(dataloader_pin_memory=False))\n",
    "        preds = trainer.predict(test_dataset)\n",
    "        predictions[model_name] = np.argmax(preds.predictions, axis=1)\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "\n",
    "# Get predictions from all models\n",
    "model_predictions = get_model_predictions(all_results, test_dataset)\n",
    "y_true = get_true_labels(test_dataset)\n",
    "\n",
    "# Add majority voting predictions\n",
    "model_predictions['Majority Voting'] = y_pred_majority\n",
    "\n",
    "# Calculate misclassification rates per class\n",
    "def calculate_misclassification_rates(y_true, y_pred, num_classes):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    misclassification_rates = []\n",
    "    \n",
    "    # for i in range(num_classes):\n",
    "    #     correct = cm[i,i]\n",
    "    #     total = sum(cm[i,:])\n",
    "    #     misclassification_rates.append(1 - (correct / total))\n",
    "    # return misclassification_rates\n",
    "    for i in range(num_classes):\n",
    "        total = cm[i, :].sum()\n",
    "        if total == 0:\n",
    "            # No samples of class i: define error rate as 0.0 (or np.nan if you prefer)\n",
    "            misclassification_rates.append(0.0)\n",
    "        else:\n",
    "            correct = cm[i, i]\n",
    "            misclassification_rates.append(1.0 - (correct / total))\n",
    "    \n",
    "    return misclassification_rates\n",
    "\n",
    "num_classes = len(class_names)\n",
    "misclassification_data = {}\n",
    "\n",
    "for model_name, y_pred in model_predictions.items():\n",
    "    misclassification_data[model_name] = calculate_misclassification_rates(y_true, y_pred, num_classes)\n",
    "\n",
    "# Create the bar plot\n",
    "plt.figure(figsize=(12, 10))\n",
    "bar_width = 0.2\n",
    "index = np.arange(num_classes)\n",
    "\n",
    "for i, (model_name, rates) in enumerate(misclassification_data.items()):\n",
    "    bars = plt.bar(index + i*bar_width, rates, bar_width, label=model_name)\n",
    "\n",
    "    # Add the value at the top of each bar\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height, f'{height:.3f}', ha='center', va='bottom')\n",
    "\n",
    "\n",
    "plt.xlabel('Disaster Categories')\n",
    "plt.ylabel('Misclassification Rate')\n",
    "plt.title('Misclassification Rates by Disaster Category and Model')\n",
    "plt.xticks(index + bar_width*1.5, class_names)\n",
    "plt.legend()\n",
    "plt.grid(True, axis='y', linestyle='--', alpha=0.7)\n",
    "plt.ylim(0, max([max(rates) for rates in misclassification_data.values()]) * 1.1)  # Add 10% headroom\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot\n",
    "plt.savefig('results/misclassification_rates.png', dpi=700, bbox_inches='tight')\n",
    "# plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbfe354-9298-4a5d-9f85-4dd7a96e1794",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
